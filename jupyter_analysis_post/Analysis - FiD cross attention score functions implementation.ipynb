{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9908fbb",
   "metadata": {},
   "source": [
    "# Analysis - FiD cross attention score functions implementation\n",
    "- FiD cross attention score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dce998",
   "metadata": {},
   "source": [
    "## CHECKING PARSER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad580a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88dfdd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accelerate transformer dependencies does not match with the Transformer dependencies\n",
    "# from accelerate import Accelerator\n",
    "# from accelerate.logging import get_logger\n",
    "# from accelerate.utils import set_seed\n",
    "import heapq\n",
    "import pathlib\n",
    "import shutil\n",
    "from FiD.src.model import FiDT5\n",
    "# from src.model import FiDEncoderForSequenceClassification\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm.auto import tqdm\n",
    "from src.data import BinaryCustomDatasetShuffle\n",
    " \n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "# import evaluate\n",
    "from util import utils\n",
    "import pickle\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModel,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "#     DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    set_seed,\n",
    "#     get_scheduler,\n",
    ")\n",
    "# from util.arguments import ModelArguments, DataTrainingArguments, CustomTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02724f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "# device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc464c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f7a415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = HfArgumentParser((ModelArguments, DataTrainingArguments, CustomTrainingArguments))\n",
    "\n",
    "# model_args, data_args, train_args = parser.parse_args_into_dataclasses([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b17e3b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(vars(model_args))\n",
    "# pprint(vars(data_args))\n",
    "# pprint(vars(train_args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa755a2",
   "metadata": {},
   "source": [
    "# Get FiD Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f6ed75",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87912412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from transformers import T5PreTrainedModel\n",
    "import copy\n",
    "# from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "292a9618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FiD.src.model import FiDT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23ae4347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FiDT5(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): EncoderWrapper(\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 1024)\n",
       "      (block): ModuleList(\n",
       "        (0): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 16)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (12): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (13): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (14): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (15): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (16): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (17): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (18): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (19): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (20): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (21): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (22): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (23): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (18): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (21): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = '/scratch/philhoon-relevance/FiD/pretrained_models/nq_reader_large'\n",
    "model = FiDT5.from_pretrained(model_path)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "618a1470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.T5Tokenizer.from_pretrained('t5-base', return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fafb5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FiD.src import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86f461a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_maxlength = 200\n",
    "n_context = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70991112",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator_function = data.Collator(text_maxlength, tokenizer, n_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c746a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example = {'id': 12, \n",
    "#            'question': \"when was the public service commission original version of the upsc set up\", \n",
    "#            'ctx': {\"id\": \"17105334\", \n",
    "#                    \"title\": \"Bihar Public Service Commission\", \n",
    "#                    \"text\": \"3 of the Regulations, 1960 the Commission was constituted with a Chairman and 10 (ten) other members. The strength of members was reduced to 6 (six) after bifurcation of the State of Bihar and the State of Jharkhand vide notification no. 7/PSC-1013/95 (Part-3) Per 8262 dated 9 October 2002 of the Personnel & Administrative Reforms Department, Bihar. Article 320 and 321 of the Constitution of India prescribes the mandate of the State Public Service Commissions, which are: a)Recruitment by conduct of Competitive Examinations/ through interviews to the services of the State Government. b)Advising the State Government on the suitability of\"}\n",
    "#           }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c953bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = '/scratch/philhoon-relevance/FiD/open_domain_data/NQ_TEST_DPR_SELECTION/strict_positive_naive_damaging_remove_damage_irrelevant_relevant.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c857c938",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_examples = data.load_data(\n",
    "    eval_data, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31a2e4d",
   "metadata": {},
   "source": [
    "## Testing 3 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e0b2b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_examples = eval_examples[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "297f1765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID : 1\n",
      "Question : who got the first nobel prize in physics\n",
      "# of ctxs : 1\n",
      "ID : 2\n",
      "Question : when is the next deadpool movie being released\n",
      "# of ctxs : 1\n",
      "ID : 3\n",
      "Question : which mode is used for short wave broadcast service\n",
      "# of ctxs : 100\n"
     ]
    }
   ],
   "source": [
    "for eg in eval_examples:\n",
    "    print(f\"ID : {eg['id']}\")\n",
    "    print(f\"Question : {eg['question']}\")\n",
    "    print(f\"# of ctxs : {len(eg['ctxs'])}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d466a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = data.Dataset(\n",
    "    eval_examples, \n",
    "    2, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12b1fe97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index : 0\n",
      "question : question: who got the first nobel prize in physics\n",
      "# passages : 1\n",
      "index : 1\n",
      "question : question: when is the next deadpool movie being released\n",
      "# passages : 1\n",
      "index : 2\n",
      "question : question: which mode is used for short wave broadcast service\n",
      "# passages : 2\n"
     ]
    }
   ],
   "source": [
    "for eg in eval_dataset:\n",
    "    print(f\"index : {eg['index']}\")\n",
    "    print(f\"question : {eg['question']}\")\n",
    "    print(f\"# passages : {len(eg['passages'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6939a390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce0ecd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_gpu_batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3377ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_sampler = SequentialSampler(eval_dataset) \n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, \n",
    "    sampler=eval_sampler, \n",
    "    batch_size=per_gpu_batch_size,\n",
    "    num_workers=10,\n",
    "    collate_fn=collator_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef336eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_iter = iter(eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1f36a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FiDT5(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): EncoderWrapper(\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 1024)\n",
       "      (block): ModuleList(\n",
       "        (0): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 16)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (12): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (13): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (14): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (15): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (16): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (17): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (18): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (19): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (20): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (21): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (22): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (23): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseReluDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (18): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (21): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "# if hasattr(model, \"module\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2789d62c",
   "metadata": {},
   "source": [
    "## overwrite_forward_crossattention\n",
    "\n",
    "```python\n",
    "def overwrite_forward_crossattention(self):\n",
    "    \"\"\"\n",
    "    Replace cross-attention forward function, only used to save\n",
    "    cross-attention scores.\n",
    "    \"\"\"\n",
    "    for mod in self.decoder.block:\n",
    "        attn = mod.layer[1].EncDecAttention\n",
    "        \n",
    "        ## overwrite forward function of EncDecAttention \n",
    "        ## Same as using functool.partial\n",
    "        attn.forward = types.MethodType(a, attn)\n",
    "        \n",
    "```\n",
    "\n",
    "## reset_score_storage\n",
    "```python\n",
    "def reset_score_storage(self):\n",
    "    \"\"\"\n",
    "    Reset score storage, only used when cross-attention scores are saved\n",
    "    to train a retriever.\n",
    "    \"\"\"\n",
    "    for mod in self.decoder.block:\n",
    "        mod.layer[1].EncDecAttention.score_storage = None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "530e41b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # 24 decoder layers\n",
    "# decoder_layers = 0\n",
    "# for mod in model.decoder.block:\n",
    "#     decoder_layers += 1\n",
    "#     # This is EncDecAttention Layer\n",
    "#     # attn = mod.layer[1].EncDecAttention\n",
    "# print(f'# of decoder_layers : {decoder_layers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f29844d",
   "metadata": {},
   "source": [
    "## Original "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38b1358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.overwrite_forward_crossattention()\n",
    "# model.reset_score_storage() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2646489",
   "metadata": {},
   "source": [
    "## Implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e83123c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.overwrite_forward_crossattention_token()\n",
    "model.reset_score_storage_token() \n",
    "model.reset_score_storage() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97775540",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_object = iter(eval_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70f73f1",
   "metadata": {},
   "source": [
    "## get_crossattention_scores\n",
    "\n",
    "```python\n",
    "    def get_crossattention_scores(self, context_mask):\n",
    "        \"\"\"\n",
    "        Cross-attention scores are aggregated to obtain a single scalar per\n",
    "        passage. This scalar can be seen as a similarity score between the\n",
    "        question and the input passage. It is obtained by averaging the\n",
    "        cross-attention scores obtained on the first decoded token over heads,\n",
    "        layers, and tokens of the input passage.\n",
    "\n",
    "        More details in Distilling Knowledge from Reader to Retriever:\n",
    "        https://arxiv.org/abs/2012.04584.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        n_passages = context_mask.size(1)\n",
    "        for mod in self.decoder.block:\n",
    "            scores.append(mod.layer[1].EncDecAttention.score_storage)\n",
    "        scores = torch.cat(scores, dim=2)\n",
    "        bsz, n_heads, n_layers, _ = scores.size()\n",
    "        # batch_size, n_head, n_layers, n_passages, text_maxlength\n",
    "        scores = scores.view(bsz, n_heads, n_layers, n_passages, -1)\n",
    "        scores = scores.masked_fill(~context_mask[:, None, None], 0.)\n",
    "        scores = scores.sum(dim=[1, 2, 4])\n",
    "        ntokens = context_mask.sum(dim=[2]) * n_layers * n_heads\n",
    "        scores = scores/ntokens\n",
    "        return scores\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58bf0257",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6d9a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "(idx, _, _, context_ids, context_mask) = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68050cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb3e2e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 200])\n",
      "torch.Size([3, 2, 200])\n"
     ]
    }
   ],
   "source": [
    "print(context_ids.shape)\n",
    "print(context_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b166314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.reset_score_storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20bc43cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "                input_ids=context_ids.cuda(),\n",
    "                attention_mask=context_mask.cuda(),\n",
    "                max_length=50,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "453fe5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossattention_scores = model.get_crossattention_scores(context_mask.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61f49ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossattention_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d039c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 9])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36adf265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', 'Wilhelm', 'Con', 'rad', 'R', 'n', 't', 'gen', '</s>']\n",
      "0\n",
      "torch.Size([9])\n",
      "Wilhelm Conrad Rntgen\n",
      "['<pad>', 'May', '18,', '2018', '</s>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "1\n",
      "torch.Size([9])\n",
      "May 18, 2018\n",
      "['<pad>', '', 'ampli', 'tude', 'modul', 'ation', '</s>', '<pad>', '<pad>']\n",
      "2\n",
      "torch.Size([9])\n",
      "amplitude modulation\n"
     ]
    }
   ],
   "source": [
    "for k, o in enumerate(outputs):\n",
    "    print(tokenizer.convert_ids_to_tokens(o))\n",
    "    ans = tokenizer.decode(o, skip_special_tokens=True)\n",
    "    print(k)\n",
    "    print(o.shape)\n",
    "    print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72e62775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_ids.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eede7bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "n_passages = context_mask.size(1)\n",
    "for mod in model.decoder.block:\n",
    "    scores.append(mod.layer[1].EncDecAttention.score_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f163249e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n",
      "torch.Size([3, 16, 1, 400])\n"
     ]
    }
   ],
   "source": [
    "for s in scores:\n",
    "    print(s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f790460",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = torch.cat(scores, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c34fee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16, 24, 400])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39f37165",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz, n_heads, n_layers, _ = test_score.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2436686f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "context_mask = context_mask.to(device='cuda')\n",
    "print(context_mask.is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bc9f35c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = test_score.view(bsz, n_heads, n_layers, n_passages, -1)\n",
    "test_score = test_score.masked_fill(~context_mask[:, None, None], 0.)\n",
    "test_score = test_score.sum(dim=[1, 2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23e13720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(test_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "42cb45d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = context_mask.sum(dim=[2]) * n_layers * n_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "07b5b8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[71808,     0],\n",
       "        [68352,     0],\n",
       "        [58368, 60672]], device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4005e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = test_score/ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3eb4a6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e1f0ec54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 200])\n"
     ]
    }
   ],
   "source": [
    "print(context_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba84811e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8066a0f8",
   "metadata": {},
   "source": [
    "### get_crossattention_scores_token\n",
    "\n",
    "\n",
    "```python\n",
    "    def get_crossattention_scores(self, context_mask):\n",
    "        \"\"\"\n",
    "        Cross-attention scores are aggregated to obtain a single scalar per\n",
    "        passage. This scalar can be seen as a similarity score between the\n",
    "        question and the input passage. It is obtained by averaging the\n",
    "        cross-attention scores obtained on the first decoded token over heads,\n",
    "        layers, and tokens of the input passage.\n",
    "\n",
    "        More details in Distilling Knowledge from Reader to Retriever:\n",
    "        https://arxiv.org/abs/2012.04584.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        n_passages = context_mask.size(1)\n",
    "        for mod in self.decoder.block:\n",
    "            scores.append(mod.layer[1].EncDecAttention.score_storage)\n",
    "        scores = torch.cat(scores, dim=2)\n",
    "        bsz, n_heads, n_layers, _ = scores.size()\n",
    "        # batch_size, n_head, n_layers, n_passages, text_maxlength\n",
    "        scores = scores.view(bsz, n_heads, n_layers, n_passages, -1)\n",
    "        scores = scores.masked_fill(~context_mask[:, None, None], 0.)\n",
    "        scores = scores.sum(dim=[1, 2, 4])\n",
    "        ntokens = context_mask.sum(dim=[2]) * n_layers * n_heads\n",
    "        scores = scores/ntokens\n",
    "        return scores\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7ce505b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 3, 16, 8, 400])\n"
     ]
    }
   ],
   "source": [
    "scores_token_lst = []\n",
    "for mod in model.decoder.block:\n",
    "    att_score_layer_lst = mod.layer[1].EncDecAttention.score_storage_token\n",
    "    att_score_layer = torch.cat(att_score_layer_lst, dim=2)\n",
    "    scores_token_lst.append(att_score_layer)\n",
    "att_score_by_token = torch.stack(scores_token_lst, dim = 0)\n",
    "print(att_score_by_token.shape)\n",
    "# Layers, batch_size, n_heads, decode_tokens, n_passages * text_maxlength "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "705202d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 3, 16, 8, 400])\n"
     ]
    }
   ],
   "source": [
    "# n_layers, batch_size, n_heads, decode_tokens, n_passages * text_maxlength \n",
    "print(att_score_by_token.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "38bed267",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers, bsz, n_head, n_dec_tokens, _ = att_score_by_token.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "039d622b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 3 16 8 400\n"
     ]
    }
   ],
   "source": [
    "print(n_layers, bsz, n_head, n_dec_tokens, _)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab9866",
   "metadata": {},
   "source": [
    "###  Permute Axis 1\n",
    "     - from : n_layers, batch_size, n_heads, decode_tokens, n_passages * text_maxlength \n",
    "     - to : batch_size, decode_tokens, n_heads, n_layers, n_passages, input_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "430a7742",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_score_by_token = att_score_by_token.permute(1, 3, 2, 0, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "53549af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 16, 24, 400])\n"
     ]
    }
   ],
   "source": [
    "print(att_score_by_token.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "24562ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_score_by_token = att_score_by_token.view(bsz, n_dec_tokens, n_head, n_layers, n_passages, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "477f65be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 16, 24, 2, 200])\n"
     ]
    }
   ],
   "source": [
    "print(att_score_by_token.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad16b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "18ecf679",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_score_by_token = att_score_by_token.masked_fill(~context_mask[:, None, None, None], 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5ab485",
   "metadata": {},
   "source": [
    "### Check validity of att_score_by_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3ff4f0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4089,  0.0537,  0.6166,  1.4100, -0.6440, -0.5910,  0.6947,  0.8202,\n",
       "          1.1297, -0.8426, -2.8562,  0.2782,  0.5422, -2.3797, -1.2527, -1.3274,\n",
       "         -3.7673, -0.7114, -2.6371, -1.8213, -1.2492, -1.6385, -1.4740, -0.0855,\n",
       "         -1.6259,  1.3126, -5.7921, -2.5179, -1.2961, -2.3236, -3.5175, -2.5686,\n",
       "         -0.7658, -0.3719, -0.4020, -1.1009, -0.9070, -1.2886,  0.4926,  0.3916,\n",
       "          1.9654,  0.3270,  0.7716, -0.2830, -1.4503, -1.4470, -0.0594,  1.1424,\n",
       "          0.6825,  3.0061, -1.1830, -1.3062, -2.0726, -2.1800, -0.5330,  0.0904,\n",
       "         -1.4817, -0.2283, -1.9703, -1.1346, -2.7439, -0.7345,  0.5759, -1.3072,\n",
       "         -2.3590,  0.0672, -1.6369,  2.6603, -0.3806, -3.3741, -1.9084, -2.1624,\n",
       "         -2.8405, -3.1403, -2.5993, -0.7304, -0.0198, -1.7688, -2.0961,  2.9047,\n",
       "         -2.8840, -2.0174,  1.0311,  2.7313,  0.0604,  0.6807, -1.8110, -2.6885,\n",
       "         -2.7305, -1.4246, -2.5593,  3.2859, -2.2351, -0.5972, -0.6372, -1.9197,\n",
       "          3.0936, -3.2779, -3.5748, -2.2186,  0.0413, -0.9347, -4.3802, -1.9614,\n",
       "          2.7326, -0.8407,  0.3894,  3.5606, -4.0115, -2.5358, -1.8669, -1.1344,\n",
       "         -1.4631,  0.0788, -0.7691, -1.3996, -1.6024, -1.0657,  2.2127, -4.1489,\n",
       "         -0.4866, -0.8840, -0.4646,  0.8864, -0.7672, -2.4528,  3.6266, -2.9840,\n",
       "         -3.1922,  3.3208,  1.0653,  0.0722, -2.9620,  0.3601, -0.5703,  1.7452,\n",
       "          5.0030,  0.9172,  3.3261,  0.9871,  0.8164,  2.4175,  0.8492,  0.0194,\n",
       "          0.2005, -1.3269, -0.1953, -0.3808,  1.3417,  3.1130,  0.5741, -0.0577,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.3665, -0.5770, -0.9362, -0.4464, -2.2328, -1.9698, -1.1967, -1.3278,\n",
       "         -0.0557, -2.7865, -4.7179, -0.2508,  0.9085, -1.0506, -1.8538, -5.4597,\n",
       "         -4.7277, -1.9303, -5.8592, -6.4239, -5.5892, -4.8906, -5.3418, -7.8973,\n",
       "          3.1084, -3.9826, -8.5615, -4.7002, -3.7566, -3.9404, -5.2442, -8.7766,\n",
       "         -4.9967, -2.9744, -4.7617, -5.3556, -4.9897, -7.7473, -5.5361,  3.5710,\n",
       "         -7.6651, -3.2999, -5.1277, -5.4012, -7.3092, -4.2696, -8.1757, -7.1199,\n",
       "          1.7021, -4.1169, -6.8159, -3.1859, -6.4013, -4.4801, -7.8945, -5.6643,\n",
       "         -3.9928, -3.5178, -5.4385, -4.8503, -5.2981,  3.4070, -4.9370, -5.2512,\n",
       "         -4.0042, -3.2371, -6.9473, -3.6826, -4.2105, -2.2377, -2.3286, -2.2915,\n",
       "         -3.1110, -3.4217, -4.9637, -5.1404, -7.5158, -3.7996, -4.2062, -3.4592,\n",
       "         -3.3689, -3.7243, -0.1521, -1.6520, -0.5163, -0.7741, -2.4869, -5.1304,\n",
       "         -4.1731, -2.0810, -1.7757, -3.9390, -1.8658, -3.4505, -3.4599, -1.9400,\n",
       "         -4.7843,  2.4428, -3.9263, -2.4357, -3.9222, -4.7989, -1.9831, -3.9164,\n",
       "         -2.4117, -2.5369, -2.1526, -3.8554, -2.2952, -3.0994, -3.2360, -2.7351,\n",
       "         -2.1921, -0.3415, -4.0185, -2.6728, -3.4457, -2.7666, -2.3884, -3.1971,\n",
       "          1.7921, -1.8320, -7.4789, -2.9030, -5.3344, -2.3419, -1.5300,  1.7301,\n",
       "         -3.6627, -2.0226, -5.5664, -4.2355, -4.9843, -3.8777, -2.8381, -1.2537,\n",
       "         -4.8061, -2.4501, -2.7317, -1.1940, -2.8112, -3.3501, -4.0278, -2.3838,\n",
       "         -2.0349, -3.7892, -3.8470,  3.0696, -4.9919, -3.7407, -3.5454, -2.9038,\n",
       "         -0.3373, -2.2065, -2.2830,  3.6304, -3.4303, -4.1605,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_score_by_token[2,-1,-1, -1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ea9e4f",
   "metadata": {},
   "source": [
    "###  Permute Axis 2\n",
    "     - from : batch_size, decode_tokens, n_heads, n_layers, n_passages, input_max_length\n",
    "     - to : batch_size, n_passages, decode_tokens, n_heads, n_layers, input_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "adcca9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 16, 24, 2, 200])\n",
      "torch.Size([3, 2, 8, 16, 24, 200])\n"
     ]
    }
   ],
   "source": [
    "print(att_score_by_token.shape)\n",
    "att_score_by_token = att_score_by_token.permute(0, 4, 1, 2, 3, 5)\n",
    "print(att_score_by_token.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6f8654",
   "metadata": {},
   "source": [
    "## Average across the Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7c1b9bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 8, 16, 24, 200])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_score_by_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ffd70861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 8, 16, 24, 200])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_score_by_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a239d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "ave_att = torch.mean(att_score_by_token, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e5c529d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 8, 24, 200])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ave_att.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "061e7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_layer = ave_att[:,:,:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6c5d09f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 8, 200])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cfffd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d34398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b47b4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "afc165b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[71808,     0],\n",
       "        [68352,     0],\n",
       "        [58368, 60672]], device='cuda:0')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81330c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eb035a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ab546805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "701538f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16, 24, 200])\n",
      "torch.Size([8, 16, 24, 200])\n"
     ]
    }
   ],
   "source": [
    "for j in range(context_ids.size(1)):\n",
    "    k_test = att_score_by_token[k, j]\n",
    "    print(k_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d155d200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 8, 16, 24, 200])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_score_by_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "46b5e2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 16, 24, 200])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_score_by_token[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f4141f",
   "metadata": {},
   "source": [
    "## Testing Distributed Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "99406593",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_score_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "807d3607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "41459370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 8, 16, 24, 200])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_score_by_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2b031152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_score_by_token[0].is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1b6fbcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "torch.Size([2, 8, 16, 24, 200])\n",
      "tensor(1)\n",
      "torch.Size([2, 8, 16, 24, 200])\n",
      "tensor(2)\n",
      "torch.Size([2, 8, 16, 24, 200])\n"
     ]
    }
   ],
   "source": [
    "for k, o in enumerate(outputs):\n",
    "    print(idx[k])\n",
    "    att_score_on_k = att_score_by_token[k].detach().cpu()\n",
    "    print(att_score_on_k.shape)\n",
    "    att_score_lst.append(\n",
    "                        {\n",
    "                            'id' : idx[k],\n",
    "                            'attention_score': att_score_on_k,\n",
    "                        }\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "627a32bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(att_score_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "39873c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[ 7.4193e+00,  7.4041e+00, -1.0412e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-5.5862e-01, -8.7128e-01, -2.2208e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-3.7667e-01,  8.3930e-03, -1.3594e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-5.9163e-01, -2.0779e+00, -1.9662e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.0526e+00, -2.2059e+00, -2.5522e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 4.7861e-01, -4.9904e-01, -4.3489e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[ 7.3228e+00,  6.9019e+00, -3.1981e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 6.4861e-01,  6.5411e-01, -2.3689e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.6608e+00, -1.8966e+00, -2.9741e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 1.7704e+00,  5.5119e-01, -1.0496e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.0581e+00, -3.1951e-01, -4.5194e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-4.3168e-01, -1.2592e+00, -1.7119e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-2.7519e+00, -2.8269e+00, -4.7170e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.2526e+00, -1.4787e+00, -1.2180e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-3.5089e-02, -2.8197e-01, -1.9261e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 9.8828e-01, -7.6345e-01, -2.9851e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.5514e-01, -1.0202e+00, -3.9258e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-5.7733e-01, -1.8310e+00, -3.6092e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[-1.3831e+00, -1.5454e+00, -2.5616e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-6.7646e-01, -8.8727e-01, -1.9742e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 3.7562e-01,  2.8259e-01, -4.2221e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 3.5804e-02, -1.1017e+00, -1.7195e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 3.5559e-01, -1.0179e+00, -9.1773e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.6962e+00,  9.0719e-01, -1.4927e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-2.1357e+00, -2.2447e+00, -2.4648e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 5.3740e-01,  6.9488e-01, -1.9979e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 4.3373e-01,  4.8055e-01,  7.3967e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 5.7794e-01, -4.4312e-01, -2.6734e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-6.2969e-01, -1.7765e+00, -2.4991e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.8817e-01, -7.8914e-01, -9.9784e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-1.9889e+00, -2.2681e+00, -1.1899e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 3.1130e+00,  4.3083e+00,  2.0763e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-6.1856e-01, -6.1819e-01, -3.2878e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-8.5386e-01, -1.9158e+00, -3.0882e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.3539e-01, -2.0051e+00, -2.8658e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.2171e+00,  6.1657e-01,  1.5299e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "         [[[-4.3588e-01, -4.6903e-01, -6.0752e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 7.3916e-01,  5.9518e-01, -3.0335e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.8206e+00, -1.6668e+00,  1.9377e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-4.4329e-01, -1.7756e+00, -1.0291e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.4004e+00, -2.0431e+00, -1.6488e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-8.1452e-01, -1.0968e+00, -4.6456e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[ 1.0170e+00,  1.3338e+00, -2.5246e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 4.8091e-02, -1.4520e-01, -1.7792e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 5.0960e-01,  1.3026e-01, -2.0084e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 5.1565e-01, -1.6940e-01, -5.9248e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 4.5925e-01, -1.2627e-01, -3.5912e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.6917e-01, -1.2346e-01, -5.9274e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-2.1981e+00, -2.5405e+00, -1.6440e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.0434e+00,  3.6457e-01, -2.1488e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.6251e+00,  1.5509e+00, -4.3410e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 6.8027e-01, -9.7192e-01, -3.3202e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-5.5654e-02, -9.1167e-01, -4.8794e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.1336e+00,  7.2702e-01, -5.0177e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[-4.7543e-01, -6.9952e-01,  1.8803e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 7.5237e+00,  7.2242e+00, -3.4884e-02,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 6.4825e-01, -9.0908e-02, -1.8108e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 1.7393e+00,  1.1631e+00, -1.8557e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 4.4042e-01, -3.9863e-01, -1.0188e-02,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.3907e-01, -1.7575e-01, -1.6688e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-1.4247e+00, -1.6533e+00, -2.1318e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.3214e+00,  1.5055e+00, -3.4553e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 4.9319e-01,  7.3569e-01,  1.7036e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-9.8361e-01, -1.7118e+00, -4.4511e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-8.4249e-01, -1.1693e+00, -2.6913e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 2.9907e-01,  4.8954e-01, -6.9887e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[ 4.3515e+00,  4.0954e+00, -3.7442e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 6.4976e+00,  7.0249e+00,  7.8047e-02,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 9.6660e-02,  6.3434e-02,  3.0251e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-1.1894e+00, -1.8622e+00, -3.8869e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.0456e+00, -4.5883e-01, -3.7584e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.6090e+00,  1.2570e+00,  1.0303e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 2.4678e+00,  2.7750e+00,  1.0489e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 4.2027e+00,  4.0787e+00, -4.7959e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 2.6580e+00,  2.4047e+00, -2.3486e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-1.0483e+00, -1.8109e+00, -4.9621e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.1846e+00, -1.9958e+00, -3.7188e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.4436e+00, -1.9885e+00, -7.1188e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-1.6560e-01, -5.2083e-01, -2.4076e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 5.8705e-01,  9.1390e-01, -3.0552e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.8451e+00,  1.2814e+00, -3.0900e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-1.1999e-01, -3.0914e-01, -2.4556e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.1617e-01, -3.5753e-01, -3.0294e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.4792e+00, -1.4284e+00, -3.6546e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[ 4.5893e-01,  4.1815e-01, -9.8080e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 6.5551e-01, -6.9189e-02, -2.5754e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-3.9746e-01, -1.4573e-01, -1.8727e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-1.3593e+00, -1.7339e+00, -1.9453e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-3.2165e+00, -3.5190e+00, -6.9225e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-3.6037e-01, -5.7157e-01, -2.1886e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 3.2719e-01,  8.5353e-02, -3.0165e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.6238e+00,  1.5220e+00, -6.0240e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.7933e+00, -2.5972e+00, -4.4946e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 6.8555e-01,  4.6748e-01, -2.8968e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-4.3968e-01, -6.2518e-01, -5.9086e-02,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.3305e+00, -1.2768e+00, -2.9256e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-2.7799e-01, -8.1483e-01, -1.0103e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.6904e+00, -1.6674e+00, -2.4542e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.0777e+00,  5.5293e-01, -7.0746e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-3.0026e+00, -2.8764e+00, -4.0312e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 3.2640e-01, -1.2630e-01, -3.3199e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.4496e+00,  2.2777e+00, -3.3196e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-3.1855e-01, -3.8973e-01,  1.8608e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.2529e+00, -7.6832e-01,  1.7933e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.1350e+00, -1.3461e+00, -2.7961e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-9.3463e-01, -1.4514e+00, -2.8230e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-3.6727e-01, -8.7445e-01, -5.2369e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 2.3856e+00,  2.4277e+00,  2.0786e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[ 3.8559e-01,  8.1911e-01,  7.9141e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 2.3327e+00,  2.2718e+00, -4.6603e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 3.5049e+00,  3.7262e+00, -1.6653e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-9.3186e-03, -1.5943e+00, -2.8539e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.6122e+00, -3.5960e+00, -2.5325e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.2152e-01, -9.0396e-01, -4.0602e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-6.7635e-01, -1.1208e+00, -4.8620e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.0476e+00,  8.1631e-01, -1.8141e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.3887e+00, -2.0185e+00, -1.5863e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 3.2561e+00,  2.5221e+00, -1.6211e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 2.3976e+00,  1.8699e+00, -3.0035e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 3.0535e+00,  2.3401e+00, -1.1187e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[ 1.7578e+00,  1.9485e+00, -3.2631e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-3.1362e-01, -2.5575e-01, -2.0605e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.7417e+00, -2.4713e+00, -2.1473e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 6.1955e-01, -3.9006e-01, -3.4664e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 3.4102e-01, -7.1005e-01, -4.3818e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.0004e-01, -7.8768e-01, -2.9314e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 7.0260e-01,  3.8955e-01, -2.4282e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 2.1801e+00,  2.0308e+00,  4.6663e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-3.2179e+00, -4.0439e+00, -2.8098e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 1.7444e+00,  1.2590e+00, -5.8099e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.1757e+00, -1.6459e+00,  7.1252e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.0519e+00,  6.9501e-01, -1.1108e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-8.1489e-01, -1.1585e+00, -3.6293e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-8.9354e-02,  6.6692e-02, -1.2443e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-3.8306e-01, -9.7473e-02,  3.8739e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-9.1711e-02, -2.9521e-01, -1.2853e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.1259e-01,  5.4031e-02, -1.2967e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-3.2502e-01, -1.4600e-01, -1.9563e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-3.9411e-01, -6.0297e-01, -5.0349e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-3.4404e+00, -3.0236e+00,  4.6017e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.8597e+00, -1.9550e+00, -2.9220e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 7.4022e-01,  5.1925e-01, -3.5874e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-8.8069e-02, -8.0112e-01, -2.4383e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 2.0854e+00,  1.7806e+00,  4.2414e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "         [[[-3.0303e-01,  8.9632e-01,  8.1516e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.1652e+00, -1.2968e+00, -5.0803e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-9.5538e-01, -1.2994e+00, -4.6356e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-2.7272e-01, -1.5362e+00, -2.4477e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.0623e+00, -2.0108e+00, -3.1497e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-8.2732e-01, -1.7116e+00, -5.2891e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-5.9743e-01, -5.8795e-01, -1.9780e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.5438e+00, -1.7777e+00, -3.4872e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.5692e+00, -2.7454e+00, -5.6247e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 1.0916e+00,  3.1595e-01, -8.9773e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.1293e+00,  4.0270e-01, -3.2229e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.4169e+00, -1.6430e+00, -2.9258e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[ 3.4701e+00,  3.4864e+00, -2.7005e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 3.2289e+00,  2.7843e+00, -1.8499e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 4.2805e-01,  5.0537e-01, -3.0132e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-1.1773e+00, -2.2741e+00, -4.6619e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-6.7699e-03, -7.6619e-01, -4.6895e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-8.3439e-01, -1.6785e+00, -2.9031e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 3.9285e-01,  1.7485e-01, -1.4842e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.3149e+00,  1.1992e+00, -2.8168e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-7.3476e-01, -2.3247e+00, -4.1822e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 1.1371e+00,  5.2317e-01, -3.4196e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.3649e+00, -3.2024e+00, -2.1133e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 6.9197e-01,  1.4231e-01, -1.7019e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-2.3734e+00, -3.5272e+00, -6.3183e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.7915e+00, -2.0708e+00, -5.9051e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 2.1148e-01,  1.1835e-01, -3.2374e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-1.5348e+00, -1.7395e+00, -2.8633e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.8456e+00, -2.0591e+00, -3.6401e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.4596e-01,  4.4579e-01, -1.6941e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[ 1.9340e+00,  1.7965e+00, -2.2791e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.6848e+00, -2.1466e+00,  1.0918e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.0100e+00, -2.3992e+00, -5.2222e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 3.3407e-01, -3.7536e-01, -2.9480e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.8418e-01, -7.8275e-01, -2.8699e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.3471e+00,  8.8267e-01,  6.1200e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 8.0231e+00,  8.0690e+00, -1.1135e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.3663e+00, -1.5826e+00, -3.0872e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.4218e+00, -1.0198e+00, -1.5562e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-1.2438e+00, -2.5861e+00, -2.4839e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.2466e+00, -2.1150e+00, -1.7413e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 8.2262e-03, -6.2120e-01, -5.1198e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[ 7.2438e+00,  6.8182e+00, -3.3361e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 8.9374e-01,  8.8730e-01, -2.6379e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.5454e+00, -2.6625e+00, -3.8416e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 2.9991e+00,  2.0512e+00, -3.5273e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.2990e+00,  2.5962e-01, -4.9813e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 2.8966e-01, -3.3846e-01, -1.3799e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-2.3400e+00, -2.3940e+00, -5.1239e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.3842e+00, -1.5050e+00, -1.6456e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.5092e+00, -1.5706e+00, -2.1947e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 1.2947e+00, -1.6190e-01, -2.9586e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 4.4836e-01, -3.1087e-01, -4.0523e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-4.0225e-01, -1.2547e+00, -3.0309e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[-1.0198e+00, -1.1441e+00, -3.2721e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-7.2270e-01, -9.1261e-01, -2.8833e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 4.6026e-02,  5.6926e-02, -6.5401e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 2.7174e-01, -6.0398e-01, -2.4884e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 3.0761e-02, -1.1822e+00, -3.6368e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.4939e+00,  1.0114e+00, -1.6197e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-2.1608e+00, -2.2659e+00, -3.4182e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 9.7430e-02,  1.2414e-01, -3.3109e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 4.9455e-01,  5.8014e-01,  6.2656e-02,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 4.6184e-01, -1.7518e-01, -2.0560e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.2294e+00, -2.0472e+00, -2.4723e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.0453e+00, -1.0225e+00, -1.5251e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-1.9206e+00, -2.0790e+00, -1.2562e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 2.6119e+00,  3.9153e+00,  2.2748e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-8.9084e-01, -8.0819e-01, -4.4222e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-4.6257e-01, -1.3284e+00, -2.8586e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 5.3921e-01, -1.3293e+00, -2.8708e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 4.0891e-01,  5.3749e-02,  6.1659e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 7.2597e+00,  7.1159e+00, -1.3421e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.1345e+00, -1.2883e+00, -2.6482e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-9.4878e-01, -5.3028e-01, -2.0886e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-1.9587e+00, -3.2865e+00, -3.9082e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.9610e+00, -2.8466e+00, -3.9644e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-7.2124e-01, -1.4673e+00, -5.8864e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[ 7.4778e+00,  7.0256e+00, -3.0882e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-9.6370e-02, -1.1465e-01, -2.5859e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.3086e+00, -2.5294e+00, -4.0768e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 6.0722e-02, -7.5280e-01, -3.1731e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-6.4809e-01, -1.6587e+00, -6.8215e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.1112e+00, -1.7076e+00, -2.9193e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-2.7638e+00, -2.7957e+00, -4.2770e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.7435e+00, -1.7344e+00, -1.3127e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-3.9468e-01, -6.5540e-01, -3.2225e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-4.3709e-01, -1.8441e+00, -4.9836e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.2681e+00, -2.0845e+00, -5.7527e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.1164e+00, -3.0386e+00, -5.6797e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[-1.9765e+00, -1.9756e+00, -2.1100e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.7338e+00, -1.8365e+00, -1.3883e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-6.8025e-01, -6.3715e-01, -1.9001e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-1.8519e+00, -2.5270e+00, -4.0122e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-9.4725e-01, -1.9764e+00, -3.6096e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 2.2645e-01, -2.0438e-01, -3.7050e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-2.3035e+00, -2.3053e+00, -2.1409e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 5.8784e-02,  1.8944e-01, -2.7075e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.4558e-01, -1.6799e-01, -2.6213e-04,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-8.5072e-01, -1.5059e+00, -5.1461e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.8672e+00, -2.6906e+00, -4.8473e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.6000e+00, -1.8368e+00, -2.9313e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-3.0381e+00, -2.9991e+00, -1.0600e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 3.0552e+00,  4.2017e+00,  1.0142e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.1545e+00, -1.1346e+00, -3.9351e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-2.1944e+00, -2.9300e+00, -5.2168e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.6465e+00, -3.1379e+00, -5.0172e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 8.6994e-02, -2.5735e-01, -5.8461e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "         [[[-5.7025e-01, -5.9264e-01, -8.2832e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-3.0808e-01, -3.0829e-01, -4.3006e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.2438e+00, -1.9663e+00, -8.8389e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-1.4364e+00, -2.6538e+00, -2.7886e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.2040e+00, -2.4967e+00, -2.3575e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.7286e+00, -1.8351e+00, -5.3309e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[ 3.8540e-01,  8.2461e-01, -3.7127e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-9.7718e-01, -1.1382e+00, -2.4703e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.9578e-01, -5.0677e-01, -2.9262e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-8.6756e-01, -1.2255e+00, -2.3269e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-7.4651e-01, -1.0025e+00, -5.0412e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-3.2694e-01, -1.9873e-01, -1.2259e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-2.8860e+00, -3.0565e+00, -2.3619e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.1499e-01, -6.6672e-01, -4.2417e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 9.6443e-01,  9.4560e-01, -2.0517e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-6.3653e-01, -1.9331e+00, -4.7349e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.0611e+00, -1.7138e+00, -5.6863e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 4.3718e-02, -1.8594e-01, -2.3602e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[-1.2903e+00, -1.3919e+00,  8.0181e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 6.1804e+00,  5.9994e+00, -1.4874e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-9.9302e-01, -1.2752e+00, -3.6187e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 5.4705e-02, -1.8190e-01, -3.8725e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-4.3171e-01, -9.3518e-01, -1.6466e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-9.5245e-01, -7.3931e-01, -3.1024e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-1.5878e+00, -1.6991e+00, -2.1414e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 7.7244e-01,  1.1672e+00, -3.5567e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-6.9351e-01, -1.6464e-01, -3.4104e-02,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-2.5734e+00, -2.9400e+00, -6.1246e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.0045e+00, -1.9341e+00, -4.5306e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-6.3909e-01, -1.6544e-01, -1.8920e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[ 3.3847e+00,  3.1743e+00, -4.6379e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 5.6247e+00,  6.2489e+00, -1.8175e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-6.1452e-01, -3.3259e-01, -2.7500e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-2.1425e+00, -2.6427e+00, -5.7041e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 3.9809e-01, -9.4754e-01, -5.1162e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.0349e+00,  8.1614e-01, -3.3831e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 2.6347e+00,  2.8608e+00,  5.4756e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 4.0376e+00,  3.8710e+00, -5.2806e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 2.0628e+00,  1.9370e+00, -3.1232e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-1.9780e+00, -2.7470e+00, -1.3801e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.6686e+00, -2.3555e+00, -5.1316e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.1828e+00, -2.6011e+00, -8.0542e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-2.5203e-01, -5.8154e-01, -2.3629e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 3.7602e-01,  4.6061e-01, -3.7129e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 8.4121e-01,  1.4422e-01, -2.1438e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-1.2855e+00, -1.2611e+00, -4.2137e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.4509e+00, -1.3952e+00, -5.0069e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.5852e+00, -1.4071e+00, -4.1277e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[ 1.2861e-01,  2.1145e-02, -1.5092e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 3.9671e-02, -6.6752e-01, -3.7394e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.4286e+00, -1.0961e+00, -4.1488e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-2.0227e+00, -2.3374e+00, -3.4720e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-3.7652e+00, -3.8646e+00, -8.4716e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.6310e+00, -1.6864e+00, -4.7725e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 7.5524e-02,  2.7240e-02, -3.2722e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.6959e-01,  1.1295e-01, -3.2712e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.9676e+00, -3.5167e+00, -6.1138e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-3.7577e-01, -3.6439e-01, -4.5754e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.4103e+00, -1.3172e+00, -2.5595e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.9965e+00, -1.7577e+00, -4.2764e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[ 2.6547e-01, -3.6434e-01, -3.7519e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.3014e+00, -2.1628e+00, -4.3742e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.7630e-01, -5.1024e-01, -3.1119e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-4.0242e+00, -3.6911e+00, -5.4769e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-6.5581e-01, -8.8852e-01, -5.1416e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 8.9975e-01,  1.8103e+00, -4.1780e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-1.1861e+00, -1.1355e+00,  1.0455e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.7504e+00, -1.1146e+00,  1.9384e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.7863e+00, -1.8540e+00, -4.2435e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-1.8146e+00, -2.2118e+00, -4.6085e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.1788e+00, -1.6852e+00, -6.3112e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.9135e+00,  2.0945e+00,  4.2057e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[ 6.5087e-01,  9.7047e-01,  8.6492e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.9845e+00,  1.9682e+00, -5.0206e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 3.0224e+00,  3.3600e+00, -2.6897e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-7.9006e-01, -2.2645e+00, -3.9562e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-3.1729e+00, -3.9645e+00, -3.9685e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-6.9899e-01, -1.4008e+00, -5.6973e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-8.5115e-01, -1.1859e+00, -7.5643e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 5.8563e-01,  3.9118e-01, -1.9270e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.4195e+00, -3.0082e+00, -3.8310e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 2.2886e+00,  1.6483e+00, -3.1708e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 8.3069e-01,  6.2731e-01, -5.2284e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 2.2091e+00,  1.6803e+00, -2.1499e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[ 1.7713e+00,  1.9191e+00, -3.3647e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-8.0366e-01, -6.0870e-01, -2.5626e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-3.0622e+00, -2.8439e+00, -3.8559e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-9.2136e-02, -9.3270e-01, -5.1198e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-4.5317e-01, -1.4333e+00, -6.3767e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.2696e+00, -2.0200e+00, -5.5168e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 6.3945e-01,  3.1506e-01, -2.9498e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 8.7997e-01,  7.7079e-01, -1.7285e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-4.1013e+00, -4.7474e+00, -4.5312e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 3.8546e-01,  6.9922e-02, -3.6518e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.7512e+00, -2.0914e+00, -9.9047e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 5.0147e-01,  1.6299e-01, -2.1329e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-4.8276e-01, -7.3633e-01, -3.0801e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-3.2316e-01, -3.6771e-01, -2.5757e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.4048e+00, -9.7000e-01,  1.2317e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-1.2758e+00, -1.2691e+00, -2.8669e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-5.6623e-01, -4.1312e-01, -2.4655e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-8.1403e-01, -6.9682e-01, -2.1244e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-8.0981e-01, -9.2289e-01, -8.9676e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-3.7127e+00, -3.2330e+00, -2.1535e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.5850e+00, -2.4472e+00, -3.9467e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-1.5660e-02, -1.0120e-01, -2.2061e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.1049e+00, -1.8097e+00, -4.7135e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 1.7857e+00,  1.4426e+00, -7.7574e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "         [[[-1.4108e-01,  9.6677e-01,  4.2720e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-8.5247e-01, -1.0646e+00, -4.8236e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-7.9154e-01, -1.0836e+00, -4.8787e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-1.1688e+00, -2.2655e+00, -3.6454e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.9090e+00, -2.5997e+00, -4.6143e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.5368e+00, -2.2191e+00, -6.6637e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-1.0211e+00, -9.0503e-01, -2.6780e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.4305e+00, -1.7517e+00, -3.3727e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.5161e+00, -2.9044e+00, -5.6525e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-2.2592e-01, -7.0986e-01, -2.5885e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-4.5966e-01, -8.2265e-01, -5.2020e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.8784e+00, -2.0798e+00, -3.3222e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[ 3.2961e+00,  3.1917e+00, -2.8157e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 3.2844e+00,  2.8759e+00, -1.6266e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 4.7037e-01,  5.3296e-01, -3.4621e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-1.9242e+00, -2.8020e+00, -6.0729e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-9.6290e-01, -1.4396e+00, -6.5736e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.1088e+00, -2.6695e+00, -5.2908e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 1.7914e-01, -2.2458e-02, -1.5625e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 9.1170e-01,  7.9486e-01, -3.5450e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.9656e+00, -2.9612e+00, -4.6079e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-2.0705e-01, -5.1066e-01, -5.9281e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.9371e+00, -3.4819e+00, -3.7324e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.5674e-01, -5.3629e-01, -2.8279e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-2.2382e+00, -3.3279e+00, -6.4657e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-9.7265e-01, -1.2570e+00, -4.9275e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.8899e-02, -6.7891e-02, -3.7236e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-2.5969e+00, -2.5111e+00, -4.1779e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.5557e+00, -2.6615e+00, -5.0656e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-4.0810e-01, -6.5483e-02, -2.3805e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[ 1.6087e+00,  1.4500e+00, -3.1126e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.3713e+00, -1.9196e+00,  6.3490e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.9940e+00, -2.3170e+00, -5.2402e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-8.7988e-01, -1.2813e+00, -4.7031e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-7.4286e-01, -1.4943e+00, -4.2727e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 5.4439e-01,  1.7212e-01, -8.4324e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 7.8918e+00,  7.7948e+00, -1.4705e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.5813e+00, -1.6950e+00, -3.0058e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.7583e+00, -1.3056e+00, -1.8987e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-2.6713e+00, -3.8432e+00, -4.1268e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.1868e+00, -2.7720e+00, -3.1695e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-9.5699e-01, -1.3964e+00, -6.3371e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[ 7.5147e+00,  7.0647e+00, -3.1562e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 2.8345e-01,  2.3539e-01, -2.5997e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.9672e+00, -3.1140e+00, -4.6952e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 1.4759e+00,  8.4149e-01, -2.2908e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-4.1639e-01, -1.0500e+00, -7.2629e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.6146e-01, -6.4425e-01, -2.1888e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-2.0292e+00, -2.0445e+00, -4.2593e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.4660e+00, -1.4221e+00, -1.1923e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.6849e+00, -1.8497e+00, -3.3329e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 3.5739e-02, -1.0455e+00, -4.6474e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-7.6201e-01, -1.2004e+00, -5.8597e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.8138e+00, -2.3242e+00, -5.0344e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[-1.3164e+00, -1.3289e+00, -2.2733e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.5427e+00, -1.6643e+00, -1.1812e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-8.6624e-01, -7.0267e-01, -1.9321e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-1.4497e+00, -1.8372e+00, -4.6013e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.0416e+00, -1.8905e+00, -2.8161e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 2.7855e-01,  1.2141e-01, -3.6142e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-2.2040e+00, -2.2070e+00, -2.9097e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.5179e-03, -5.3317e-02, -3.5831e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-3.8256e-02,  5.0134e-02, -4.8352e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-9.7272e-01, -1.1722e+00, -4.1202e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.2624e+00, -2.8145e+00, -4.6513e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-2.1895e+00, -1.8636e+00, -2.7759e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "          [[-2.5082e+00, -2.4553e+00, -6.7458e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [ 2.8219e+00,  4.0339e+00,  1.4378e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-1.2640e+00, -1.2305e+00, -4.8292e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [-1.6246e+00, -2.1823e+00, -4.9919e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-8.4953e-01, -2.4287e+00, -4.6648e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-3.6653e-01, -5.7696e-01, -9.3619e-01,  ...,  0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00]]]]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_score_lst[2]['attention_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c682ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_name = 'opt_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c535a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27870a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = Path('/home/philhoon/relevance_retrieval/test_pickle') / opt_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a12d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec418a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path = dir_path / 'tmp_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4d857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2323b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be87a265",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path = write_path / 'test-0.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1716eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7f7c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926c273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tmp_path, 'wb') as fw:\n",
    "    pickle.dump(att_score_lst, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77382961",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_path = dir_path / 'attention_score.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c294ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d99cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_path = write_path / '*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b357ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5a9146",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = write_path.glob('*.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5309b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in results_path:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = []\n",
    "for path in results_path:\n",
    "    print(path)\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    print(len(data))\n",
    "    alldata.extend(data)\n",
    "    path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937d5928",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(alldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d275be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ins in alldata:\n",
    "    print(ins['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988b9d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(final_path, 'wb') as fout:\n",
    "    pickle.dump(alldata, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a794f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path.rmdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a010e2e9",
   "metadata": {},
   "source": [
    "## Checking TEST output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2067f47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = '/data/philhoon-relevance/FiD/results/NQ_DPR/DEV_ATT_TOKEN/NQ_dev_1_context/attention_score.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0f44d0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pickle_file, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7696bad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8757"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "baacf25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'attention_score'])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[100].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "392e0c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 16, 24, 200])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[100]['attention_score'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "98adc0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_in_bytes = data[100]['attention_score'].nelement() * data[100]['attention_score'].element_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f62cce7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4300800"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_in_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b45a0fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4200.0"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_in_bytes/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ea19c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = 0\n",
    "for ins in data:\n",
    "    size_ = ins['attention_score'].nelement() * ins['attention_score'].element_size()\n",
    "    total_size += size_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7fc8bc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42174566400\n"
     ]
    }
   ],
   "source": [
    "print(total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655abd2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e9d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1453a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9654d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fbde5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lst.append(\n",
    "    {\n",
    "        'id' : idx[k],\n",
    "        'attention_score' : att_score_by_token,\n",
    "     \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fbb7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lst[0]['attention_score'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e1b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a7f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers, batch_size, n_heads, , n_passages * text_maxlength \n",
    "# batch_size, decode_tokens, n_heads, Layers, n_passages * text_maxlength  \n",
    "score_by_token = score_by_token.permute(1, 3, 2, 0, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1413bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, decode_tokens, n_heads, Layers, n_passages * text_maxlength  \n",
    "score_by_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68084023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, decode_tokens, n_heads, Layers, n_passages, text_maxlength \n",
    "# score_by_token = score_by_token.view(bsz, decoder_token, n_heads, n_layers, n_passages, -1)\n",
    "score_by_token = score_by_token.view(1, 8, 16, 24, 2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3685646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_by_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dfc14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94334564",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_mask[:, None, None, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d3090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_by_token = score_by_token.masked_fill(~context_mask[:, None, None, None], 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea8f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, decoded_token, n_head, n_layers, n_passages, text_max_length\n",
    "score_by_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c4b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum across n_head, n_layers\n",
    "score_by_token = score_by_token.sum(dim=[2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e653e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, decoded_token, n_passages, text_max_length\n",
    "score_by_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f6a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average over n_head * n_layers\n",
    "n_ = 16 * 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a20dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_by_token = score_by_token/n_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a00f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_by_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f3446c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# BATCH, decode tokens, context, encoder tokens\n",
    "for i in range(0,8):\n",
    "    print(score_by_token[0,2,1,:].shape)\n",
    "    print(score_by_token[0,2,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cc6583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64afa54f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e2603d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81af687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50defb63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c62f9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71386f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f56d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_mask[:,None, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0e0cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e190771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25acb69e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5108a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc388f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in scores:\n",
    "    print(s.shape)\n",
    "scores = torch.cat(scores, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a92c90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size, n_heads, n_layers, n_passages\n",
    "scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3744a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz, n_heads, n_layers, _ = scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e004dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = scores.view(bsz, n_heads, n_layers, n_passages, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fb7f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_mask = context_mask.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd80f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context_mask.is_cuda)\n",
    "print(scores.is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2391eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67542492",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_mask[:, None, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e340ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = scores.masked_fill(~context_mask[:, None, None], 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a29b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, n_heads, n_layers, n_passages, text_maxlength\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cedf264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum across - n_heads, n_layers, text_maxlength\n",
    "scores = scores.sum(dim=[1, 2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3608baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, n_passages\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3e306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum across =tokens\n",
    "context_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e27ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_mask.sum(dim=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc382bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = context_mask.sum(dim=[2]) * n_layers * n_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b91ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7643bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = scores/ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b752470",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b12532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ca759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_mask.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461df0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960a5772",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22057294",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = scores[:, None, : , None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7216bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870cce81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590b999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0750cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_by_encoder_token_lst = []\n",
    " \n",
    "for token_ind, s_ in enumerate(scores_token_lst, 0):\n",
    "    print(f'decoded token {token_ind}')\n",
    "    print(len(s_))\n",
    "    print((s_[1].size()))\n",
    "    scores = torch.cat(s_, dim=2)\n",
    "    scores_by_encoder_token_lst.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112da938",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_by_encoder_token_lst[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76d0add",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7cb277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883281e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549c061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39eba7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6082b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7058b766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b90c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedfb1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c781a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scores_token_lst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387e0221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c52e3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c5173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens([    0, 27545,  1193,  5672, 22087,    29,    17,   729,     1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c64572d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed08e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossattention_scores = model.get_crossattention_scores(context_mask.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e2f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross attention score on passage level\n",
    "# crossattention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f3091",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, o in enumerate(outputs):\n",
    "    print(k)\n",
    "    print(o.shape)\n",
    "    print(o)\n",
    "    ans = tokenizer.decode(o, skip_special_tokens=False)\n",
    "#     print(tokenizer.decode([0,1], skip_special_tokens=False))\n",
    "    print(ans)\n",
    "#     ans = tokenizer.decode(o, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01b1c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "n_passages = context_mask.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab750e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "n_passages = context_mask.size(1)\n",
    "for mod in self.decoder.block:\n",
    "    scores.append(mod.layer[1].EncDecAttention.score_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ab6c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc8521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_scores = 0\n",
    "for score in scores:\n",
    "    cnt_scores += 1\n",
    "    print(score.shape)\n",
    "# batch_size, n_head, n_layers, n_passages, text_maxlength\n",
    "print(cnt_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88a578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.cat(scores, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80973a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, n_head, n_layers, n_passages * text_maxlength ->  batch_size, n_head, n_layers, n_passages, text_maxlength\n",
    "print(scores.shape)\n",
    "scores = scores.view(1, 16, 24, 2, -1)\n",
    "print(scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98865c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad66b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context_mask.shape)\n",
    "print(context_mask[:, None, None].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5951c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_mask = context_mask.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e362199",
   "metadata": {},
   "outputs": [],
   "source": [
    "~context_mask[:,None,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af957549",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = scores.masked_fill(~context_mask[:, None, None], 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b864662",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a55a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d561b1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3070fddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09a501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1941bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b9635d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dce130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305af5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152fd005",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        (idx, _, _, context_ids, context_mask) = batch\n",
    "\n",
    "        model.reset_score_storage()\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids=context_ids.cuda(),\n",
    "            attention_mask=context_mask.cuda(),\n",
    "            max_length=50,\n",
    "        )\n",
    "\n",
    "        if opt.write_crossattention_scores:\n",
    "            crossattention_scores = model.get_crossattention_scores(context_mask.cuda())\n",
    "\n",
    "        for k, o in enumerate(outputs):\n",
    "            ans = tokenizer.decode(o, skip_special_tokens=True)\n",
    "            example = dataset.data[idx[k]]\n",
    "            if 'answers' in example:\n",
    "                score = src.evaluation.ems(ans, example['answers'])\n",
    "                exactmatch.append(score)\n",
    "\n",
    "            if opt.write_results:\n",
    "                fw.write(str(example['id']) + \"\\t\" + ans  + \"\\t\" + str(score) + '\\n')\n",
    "            if opt.write_crossattention_scores:\n",
    "                for j in range(context_ids.size(1)):\n",
    "                    example['ctxs'][j]['score'] = crossattention_scores[k, j].item()\n",
    "\n",
    "            total += 1\n",
    "        if (i + 1) % opt.eval_print_freq == 0:\n",
    "            log = f'Process rank:{opt.global_rank}, {i+1} / {len(dataloader)}'\n",
    "            if len(exactmatch) == 0:\n",
    "                log += '| no answer to compute scores'\n",
    "            else:\n",
    "                log += f' | average = {np.mean(exactmatch):.3f}'\n",
    "            logger.warning(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f1d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34098c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f2fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd104331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8144c60c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d0bcff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19adf450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590d8cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59f8b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472c5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a604ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f028e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc92b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950978cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn.forward = types.MethodType(cross_attention_forward, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4497b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b6b5b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model.encoder => FiDT5.EncoderWrapper\n",
    "# model.encoder.encoder => FiDT5.EncoderWrapper.encoder = T5 encoder Architecture w FiDT5 parameters\n",
    "model_encoder = model.encoder.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5200122",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = AutoTokenizer.from_pretrained('t5-base', return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862f68b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3620652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8b3f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer(example['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c62a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = np.expand_dims(np.array(output['input_ids']), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2957fce2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_ids = torch.from_numpy(test_ids)\n",
    "print(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a93a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_attention = np.expand_dims(np.array(output['attention_mask']), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a316a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_attention = torch.from_numpy(test_attention)\n",
    "print(test_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cfd3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model_encoder.forward(input_ids = test_ids, attention_mask = test_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc662c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4d7161",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['last_hidden_state'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29acf2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3fb576",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.d_model, config.d_model)\n",
    "        classifier_dropout = (\n",
    "            config.dropout_rate\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.out_proj = nn.Linear(config.d_model, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab03778",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptivePooler(nn.Module):\n",
    "    \"\"\" Calcualte weighted average of the inputs with learnable weights \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.input_size = config.d_model\n",
    "        self.w = nn.Linear(self.input_size, 1, bias=True)\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "        self.classifier = nn.Linear(config.d_model, config.num_labels)\n",
    "\n",
    "    def forward(self, inputs, mask=None):\n",
    "        batch_size, seq_len, emb_dim = inputs.shape\n",
    "        scores = torch.squeeze(self.w(inputs), dim=-1)\n",
    "        weights = nn.functional.softmax(scores, dim=-1)\n",
    "        if mask is not None:\n",
    "            weights = weights * mask\n",
    "            weights = weights / weights.sum(dim=-1, keepdims=True)\n",
    "        outputs = (inputs.permute(2, 0, 1) * weights).sum(-1).T\n",
    "        \n",
    "        outputs = self.dropout(outputs)\n",
    "        logits = self.classifier(outputs)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d168ac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_test(module):\n",
    "    \"\"\" Initialize the weights \"\"\"\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "        module.weight.data.zero_()\n",
    "       \n",
    "    if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "        module.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb6cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptivePooler_.apply(init_weights_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79421f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptivePooler_.w.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f74c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.problem_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6511cfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiDEncoderForSequenceClassification(nn.Module):\n",
    "    def __init__(self, config, model_encoder, pooler='adaptive'):\n",
    "        super(FiDEncoderForSequenceClassification, self).__init__()\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "        \n",
    "        self.encoder = model_encoder\n",
    "        \n",
    "        classification_class = AdaptivePooler if pooler == 'adaptive' else ClassificationHead\n",
    "        self.classifier = classification_class(self.config)\n",
    "        \n",
    "        self.classifier.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize the weights \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=config.initializer_factor)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "            \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
    "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        logits = self.classifier(outputs[0], mask=attention_mask)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        \n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599907b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_classifier = FiDEncoderForSequenceClassification(config, model_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fedff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = encoder_classifier.forward(input_ids = test_ids, attention_mask = test_attention, return_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75067c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3186da",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"layer_norm.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365c2874",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = [p for n, p in encoder_classifier.named_parameters() if not any(nd in n for nd in no_decay)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdd34fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d5efb3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for n, p in model.named_parameters() :\n",
    "    if any(nd in n for nd in no_decay):\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aef95e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0208dbf",
   "metadata": {},
   "source": [
    "# Checking Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c89525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric_pre = evaluate.load('precision')\n",
    "metric_re = evaluate.load('recall')\n",
    "metric_f1 = evaluate.load('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081a751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([1,1,1,1,1])\n",
    "references = np.array([1,1,1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b0fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_acc.add_batch(\n",
    "                predictions=predictions,\n",
    "                references=references,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e758554",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric = metric_acc.compute()\n",
    "print(eval_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7013ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric = metric_acc.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c23d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c4f6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19367c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fdde46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5245c917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a580903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import numpy as np\n",
    "\n",
    "class FiDT5(transformers.T5ForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.wrap_encoder()\n",
    "\n",
    "    def forward_(self, **kwargs):\n",
    "        if 'input_ids' in kwargs:\n",
    "            kwargs['input_ids'] = kwargs['input_ids'].view(kwargs['input_ids'].size(0), -1)\n",
    "        if 'attention_mask' in kwargs:\n",
    "            kwargs['attention_mask'] = kwargs['attention_mask'].view(kwargs['attention_mask'].size(0), -1)\n",
    "\n",
    "        return super(FiDT5, self).forward(\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    # We need to resize as B x (N * L) instead of (B * N) x L here\n",
    "    # because the T5 forward method uses the input tensors to infer\n",
    "    # dimensions used in the decoder.\n",
    "    # EncoderWrapper resizes the inputs as (B * N) x L.\n",
    "    def forward(self, input_ids=None, attention_mask=None, **kwargs):\n",
    "        if input_ids != None:\n",
    "            # inputs might have already be resized in the generate method\n",
    "            if input_ids.dim() == 3:\n",
    "                self.encoder.n_passages = input_ids.size(1)\n",
    "            input_ids = input_ids.view(input_ids.size(0), -1)\n",
    "        if attention_mask != None:\n",
    "            attention_mask = attention_mask.view(attention_mask.size(0), -1)\n",
    "        return super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    # We need to resize the inputs here, as the generate method expect 2D tensors\n",
    "    def generate(self, input_ids, attention_mask, max_length):\n",
    "        self.encoder.n_passages = input_ids.size(1)\n",
    "        return super().generate(\n",
    "            input_ids=input_ids.view(input_ids.size(0), -1),\n",
    "            attention_mask=attention_mask.view(attention_mask.size(0), -1),\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "    def wrap_encoder(self, use_checkpoint=False):\n",
    "        \"\"\"\n",
    "        Wrap T5 encoder to obtain a Fusion-in-Decoder model.\n",
    "        \"\"\"\n",
    "        self.encoder = EncoderWrapper(self.encoder, use_checkpoint=use_checkpoint)\n",
    "\n",
    "    def unwrap_encoder(self):\n",
    "        \"\"\"\n",
    "        Unwrap Fusion-in-Decoder encoder, useful to load T5 weights.\n",
    "        \"\"\"\n",
    "        self.encoder = self.encoder.encoder\n",
    "        block = []\n",
    "        for mod in self.encoder.block:\n",
    "            block.append(mod.module)\n",
    "        block = nn.ModuleList(block)\n",
    "        self.encoder.block = block\n",
    "\n",
    "    def load_t5(self, state_dict):\n",
    "        self.unwrap_encoder()\n",
    "        self.load_state_dict(state_dict)\n",
    "        self.wrap_encoder()\n",
    "\n",
    "    def set_checkpoint(self, use_checkpoint):\n",
    "        \"\"\"\n",
    "        Enable or disable checkpointing in the encoder.\n",
    "        See https://pytorch.org/docs/stable/checkpoint.html\n",
    "        \"\"\"\n",
    "        for mod in self.encoder.encoder.block:\n",
    "            mod.use_checkpoint = use_checkpoint\n",
    "\n",
    "    def reset_score_storage(self):\n",
    "        \"\"\"\n",
    "        Reset score storage, only used when cross-attention scores are saved\n",
    "        to train a retriever.\n",
    "        \"\"\"\n",
    "        for mod in self.decoder.block:\n",
    "            mod.layer[1].EncDecAttention.score_storage = None\n",
    "\n",
    "    def get_crossattention_scores(self, context_mask):\n",
    "        \"\"\"\n",
    "        Cross-attention scores are aggregated to obtain a single scalar per\n",
    "        passage. This scalar can be seen as a similarity score between the\n",
    "        question and the input passage. It is obtained by averaging the\n",
    "        cross-attention scores obtained on the first decoded token over heads,\n",
    "        layers, and tokens of the input passage.\n",
    "\n",
    "        More details in Distilling Knowledge from Reader to Retriever:\n",
    "        https://arxiv.org/abs/2012.04584.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        n_passages = context_mask.size(1)\n",
    "        for mod in self.decoder.block:\n",
    "            scores.append(mod.layer[1].EncDecAttention.score_storage)\n",
    "        scores = torch.cat(scores, dim=2)\n",
    "        bsz, n_heads, n_layers, _ = scores.size()\n",
    "        # batch_size, n_head, n_layers, n_passages, text_maxlength\n",
    "        scores = scores.view(bsz, n_heads, n_layers, n_passages, -1)\n",
    "        scores = scores.masked_fill(~context_mask[:, None, None], 0.)\n",
    "        scores = scores.sum(dim=[1, 2, 4])\n",
    "        ntokens = context_mask.sum(dim=[2]) * n_layers * n_heads\n",
    "        scores = scores/ntokens\n",
    "        return scores\n",
    "\n",
    "    def overwrite_forward_crossattention(self):\n",
    "        \"\"\"\n",
    "        Replace cross-attention forward function, only used to save\n",
    "        cross-attention scores.\n",
    "        \"\"\"\n",
    "        for mod in self.decoder.block:\n",
    "            attn = mod.layer[1].EncDecAttention\n",
    "            attn.forward = types.MethodType(cross_attention_forward, attn)\n",
    "\n",
    "class EncoderWrapper(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Wrapper for T5 Wrapper to obtain a Fusion-in-Decoder model.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, use_checkpoint=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        apply_checkpoint_wrapper(self.encoder, use_checkpoint)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, **kwargs,):\n",
    "        # total_length = n_passages * passage_length\n",
    "        bsz, total_length = input_ids.shape\n",
    "        passage_length = total_length // self.n_passages\n",
    "        input_ids = input_ids.view(bsz*self.n_passages, passage_length)\n",
    "        attention_mask = attention_mask.view(bsz*self.n_passages, passage_length)\n",
    "        outputs = self.encoder(input_ids, attention_mask, **kwargs)\n",
    "        outputs = (outputs[0].view(bsz, self.n_passages*passage_length, -1), ) + outputs[1:]\n",
    "        return outputs\n",
    "\n",
    "class CheckpointWrapper(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper replacing None outputs by empty tensors, which allows the use of\n",
    "    checkpointing.\n",
    "    \"\"\"\n",
    "    def __init__(self, module, use_checkpoint=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, position_bias, **kwargs):\n",
    "        if self.use_checkpoint and self.training:\n",
    "            kwargs = {k: v for k, v in kwargs.items() if v is not None}\n",
    "            def custom_forward(*inputs):\n",
    "                output = self.module(*inputs, **kwargs)\n",
    "                empty = torch.tensor(\n",
    "                    [],\n",
    "                    dtype=torch.float,\n",
    "                    device=output[0].device,\n",
    "                    requires_grad=True)\n",
    "                output = tuple(x if x is not None else empty for x in output)\n",
    "                return output\n",
    "\n",
    "            output = torch.utils.checkpoint.checkpoint(\n",
    "                custom_forward,\n",
    "                hidden_states,\n",
    "                attention_mask,\n",
    "                position_bias\n",
    "            )\n",
    "            output = tuple(x if x.size() != 0 else None for x in output)\n",
    "        else:\n",
    "            output = self.module(hidden_states, attention_mask, position_bias, **kwargs)\n",
    "        return output\n",
    "\n",
    "def apply_checkpoint_wrapper(t5stack, use_checkpoint):\n",
    "    \"\"\"\n",
    "    Wrap each block of the encoder to enable checkpointing.\n",
    "    \"\"\"\n",
    "    block = []\n",
    "    for mod in t5stack.block:\n",
    "        wrapped_mod = CheckpointWrapper(mod, use_checkpoint)\n",
    "        block.append(wrapped_mod)\n",
    "    block = nn.ModuleList(block)\n",
    "    t5stack.block = block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae05859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194aa016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d00552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535af12f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97b3fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facb1d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6681191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445c430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9945bd16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec20fa66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e91b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0518e446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e558e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8055b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import BinaryCustomDatasetShuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7292457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import evaluate\n",
    "from util import utils\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoModel, \n",
    "    AutoConfig, \n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    get_scheduler,\n",
    ")\n",
    "from util.arguments import ModelArguments, DataTrainingArguments \n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa92fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCustomDatasetShuffle(torch.utils.data.Dataset):\n",
    "    def __init__(self, instances, tokenizer, max_length, shuffle = False):\n",
    "        if shuffle:\n",
    "            random.shuffle(instances)\n",
    "        self.instances = instances\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token = tokenizer.sep_token\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = 'question: ' + self.instances[idx]['question'] + \\\n",
    "                 ' title: ' + self.instances[idx]['ctx']['title'] + \\\n",
    "                 ' context : ' + self.instances[idx]['ctx']['text']\n",
    "        output = self.tokenizer(\n",
    "            input_,\n",
    "            # return_tensors=\"pt\", will be applied later through collator\n",
    "            # padding=True, will be padded later through collate\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length)\n",
    "\n",
    "        item = {key: val for key, val in output.items()}\n",
    "        # item['labels'] = torch.tensor(int(self.instances[idx]['em']))\n",
    "        item['labels'] = int(self.instances[idx]['em'])\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b9ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8b9c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_to = 'wandb'\n",
    "output_dir = '/data/philhoon-relevance/binary-classification/results/NQ-DEV-DPR/5-fold/1/scratch_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ec03c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerator = Accelerator(log_with=args.report_to, logging_dir=args.output_dir) if args.with_tracking else Accelerator()\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd47bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(logging_dir=output_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8286be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c47e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = 'roberta-large'\n",
    "num_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed884cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_name_or_path, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d8ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f384d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ed707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f15201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_mismatched_sizes = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba87a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        config=config,\n",
    "        ignore_mismatched_sizes=ignore_mismatched_sizes,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e89593",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '/data/philhoon-relevance/binary-classification/\\\n",
    "NQ-DEV-DPR/5-fold/1/binary_data/binary_ex_ctx100id_split_train_1_partial.json'\n",
    "eval_file = '/data/philhoon-relevance/binary-classification/\\\n",
    "NQ-DEV-DPR/5-fold/1/binary_data/binary_ex_ctx100id_split_train_1_partial.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776df27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = utils.open_json(train_file)\n",
    "eval_data = utils.open_json(eval_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fde6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 200\n",
    "shuffle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec963609",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BinaryCustomDatasetShuffle(train_data, tokenizer = tokenizer, \\\n",
    "                                           max_length = max_length, shuffle = shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5520a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = BinaryCustomDatasetShuffle(eval_data, tokenizer = tokenizer, \\\n",
    "                                           max_length = max_length, shuffle = shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab0575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47026d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e92816",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              shuffle = True,\n",
    "                              collate_fn=data_collator,\n",
    "                              batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebf4417",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = DataLoader(eval_dataset,\n",
    "                              shuffle = True,\n",
    "                              collate_fn=data_collator,\n",
    "                              batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c34cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdd98be",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a77035",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d2a0d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer_grouped_parameters[0][\"weight_decay\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c1f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d5fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba674bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler_type='linear'\n",
    "num_warmup_steps = 0\n",
    "# max_train_steps = \n",
    "num_train_epochs = 5\n",
    "gradient_accumulation_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b1c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f8b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_steps = num_train_epochs * num_update_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81266a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8961a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = get_scheduler(\n",
    "        name=lr_scheduler_type,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=max_train_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d33b38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf3477",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc85ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_train_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch_size = per_device_train_batch_size * accelerator.num_processes * gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c96172",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d7d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b459680",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d1b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_epoch = 0\n",
    "with_tracking = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25897061",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointing_steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c5778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(starting_epoch, num_train_epochs):\n",
    "    model.train()\n",
    "    if with_tracking:\n",
    "        total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        if with_tracking:\n",
    "            total_loss += loss.detach().float()\n",
    "            \n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        \n",
    "        if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "            \n",
    "        if isinstance(checkpointing_steps, int):\n",
    "            if completed_steps % checkpointing_steps == 0:\n",
    "                output_dir = f\"step_{completed_steps }\"\n",
    "                if output_dir is not None:\n",
    "                    output_dir = os.path.join(args.output_dir, output_dir)\n",
    "                accelerator.save_state(output_dir)\n",
    "        if completed_steps >= args.max_train_steps:\n",
    "                break\n",
    "                \n",
    "                \n",
    "    model.eval()\n",
    "    samples_seen = 0\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "         with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1) \n",
    "        predictions, references = accelerator.gather((predictions, batch[\"labels\"]))\n",
    "        \n",
    "        if accelerator.num_processes > 1:\n",
    "            if step == len(eval_dataloader) - 1:\n",
    "                predictions = predictions[: len(eval_dataloader.dataset) - samples_seen]\n",
    "                references = references[: len(eval_dataloader.dataset) - samples_seen]\n",
    "            else:\n",
    "                samples_seen += references.shape[0]\n",
    "        \n",
    "        metric.add_batch(\n",
    "                predictions=predictions,\n",
    "                references=references,\n",
    "            )\n",
    "        \n",
    "        eval_metric = metric.compute()\n",
    "        logger.info(f\"epoch {epoch}: {eval_metric}\")\n",
    "        \n",
    "        if args.with_tracking:\n",
    "            accelerator.log(\n",
    "                {\n",
    "                    \"accuracy\" : eval_metric,\n",
    "                    \"train_loss\": total_loss.item() / len(train_dataloader),\n",
    "                    \"epoch\": epoch,\n",
    "                    \"step\": completed_steps,\n",
    "                },\n",
    "                step=completed_steps,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f588f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric_pre = evaluate.load('precision')\n",
    "metric_re = evaluate.load('recall')\n",
    "metric_f1 = evaluate.load('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448447fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557cbe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.num_processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18636963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7522dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(\n",
    "    (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    ")\n",
    "args = [\"--model_name_or_path\", 'allenai/longformer-large-4096', '--output_dir', './']\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cab07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting last checkpoint.\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e57646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed before initializing model.\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ebdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        num_labels=model_args.num_labels,\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    )\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df813504",
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_args.do_train:\n",
    "    instances, cut_off, total_questions = preprocessing_data(\n",
    "        data_args.train_file, \n",
    "        data_args.sample_size, \n",
    "        data_args.position)\n",
    "    \n",
    "    train_instance = instances[data_args.dev_size:]\n",
    "    dev_instance = instances[:data_args.dev_size]\n",
    "    \n",
    "    train_dataset = CustomDataset(train_instance, \n",
    "                               tokenizer, \n",
    "                               model_args.max_seq_length)\n",
    "    dev_dataset = CustomDataset(train_instance, \n",
    "                               tokenizer, \n",
    "                               model_args.max_seq_length)\n",
    "    \n",
    "    # Log a few random samples from the training set:\n",
    "    for index in random.sample(range(len(train_dataset)), 3):\n",
    "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "if training_args.do_eval:\n",
    "    instances, cut_off, total_questions = preprocessing_data(\n",
    "        data_args.test_file, \n",
    "        data_args.sample_size, \n",
    "        data_args.position)\n",
    "    \n",
    "    test_dataset = CustomDataset(instances, \n",
    "                               tokenizer, \n",
    "                               model_args.max_seq_length)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7794c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metric function\n",
    "metric = evaluate.load(\"xnli\")\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return metric.compute(predictions=preds, references=p.label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c2f58",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Initialize Trainer\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer, \n",
    "    pad_to_multiple_of=8,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_train else None,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=30)]\n",
    ")\n",
    "\n",
    "# Training\n",
    "if training_args.do_train:\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    metrics = train_result.metrics\n",
    "    max_train_samples = (\n",
    "        data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
    "    )\n",
    "    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    \n",
    "# Evaluation\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "    metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "\n",
    "    max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n",
    "    metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe7e951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930ab04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0fc78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b9adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df12bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args.dataset_name = a\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633d9b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed523bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737a498d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e978cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922d1e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = HfArgumentParser(\n",
    "        (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    "    )\n",
    "    \n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FiD3",
   "language": "python",
   "name": "fid3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
