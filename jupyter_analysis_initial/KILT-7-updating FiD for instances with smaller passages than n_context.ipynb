{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e86601ea",
   "metadata": {},
   "source": [
    "# KILT-7-new dataset loader\n",
    "- current dataset loader in FiD does not work well with there are less passages than designated n_context\n",
    "- need to fix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794dd775",
   "metadata": {},
   "source": [
    "# Error Code\n",
    "\n",
    "- RuntimeError: Sizes of tensors must match except in dimension 0. Expected size 5 but got size 1 for tensor numbe\n",
    "r 2 in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c93a284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c3109b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Collator(object):\n",
    "#     def __init__(self, text_maxlength, tokenizer, answer_maxlength=20):\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.text_maxlength = text_maxlength\n",
    "#         self.answer_maxlength = answer_maxlength\n",
    "\n",
    "#     def __call__(self, batch):\n",
    "#         assert(batch[0]['target'] != None)\n",
    "#         index = torch.tensor([ex['index'] for ex in batch])\n",
    "#         target = [ex['target'] for ex in batch]\n",
    "        \n",
    "#         target = self.tokenizer.batch_encode_plus(\n",
    "#             target,\n",
    "#             max_length=self.answer_maxlength if self.answer_maxlength > 0 else None,\n",
    "#             pad_to_max_length=True,\n",
    "#             return_tensors='pt',\n",
    "#             truncation=True if self.answer_maxlength > 0 else False,\n",
    "#         )\n",
    "        \n",
    "#         target_ids = target[\"input_ids\"]\n",
    "#         target_mask = target[\"attention_mask\"].bool()\n",
    "#         target_ids = target_ids.masked_fill(~target_mask, -100)\n",
    "        \n",
    "#         def append_question(example):\n",
    "#             if example['passages'] is None:\n",
    "#                 return [example['question']]\n",
    "#             return [example['question'] + \" \" + t for t in example['passages']]\n",
    "#         text_passages = [append_question(example) for example in batch]\n",
    "#         passage_ids, passage_masks = encode_passages(text_passages,\n",
    "#                                                      self.tokenizer,\n",
    "#                                                      self.text_maxlength)\n",
    "\n",
    "#         return (index, target_ids, target_mask, passage_ids, passage_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ec19518",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator(object):\n",
    "    def __init__(self, text_maxlength, tokenizer, n_context, answer_maxlength=20):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_maxlength = text_maxlength\n",
    "        self.answer_maxlength = answer_maxlength\n",
    "        self.n_context = n_context\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        assert(batch[0]['target'] != None)\n",
    "        index = torch.tensor([ex['index'] for ex in batch])\n",
    "        target = [ex['target'] for ex in batch]\n",
    "        \n",
    "#         print(f'batch : {len(batch)}')\n",
    "#         print('target')\n",
    "#         pprint(target)\n",
    "        target = self.tokenizer.batch_encode_plus(\n",
    "            target,\n",
    "            max_length=self.answer_maxlength if self.answer_maxlength > 0 else None,\n",
    "            pad_to_max_length=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True if self.answer_maxlength > 0 else False,\n",
    "        )\n",
    "        \n",
    "#         print(f'target')\n",
    "#         pprint(target)\n",
    "        target_ids = target[\"input_ids\"]\n",
    "#         print(f'target_ids : {target_ids}')\n",
    "#         print(f'target_attention_mask')\n",
    "#         print(f'{target[\"attention_mask\"]}')\n",
    "        target_mask = target[\"attention_mask\"].bool()\n",
    "#         print(f'target_mask : {target_mask}')\n",
    "        \n",
    "        target_ids = target_ids.masked_fill(~target_mask, -100)\n",
    "#         print(f'target_ids : {target_ids}')\n",
    "\n",
    "        \n",
    "        \n",
    "#         print(example)\n",
    "#         pprint(example)\n",
    "        def append_question(example):\n",
    "            if example['passages'] is None:\n",
    "                return [example['question']]\n",
    "            return [example['question'] + \" \" + t for t in example['passages']]\n",
    "        \n",
    "        text_passages = []\n",
    "        \n",
    "        for example in batch:\n",
    "#             if len(example) > 5:\n",
    "#                 print('example')\n",
    "#                 pprint(example)\n",
    "            text_passages.append(append_question(example))\n",
    "#         text_passages = [append_question(example) for example in batch]\n",
    "        \n",
    "#         print('text_passages')\n",
    "#         pprint(text_passages)\n",
    "        passage_ids, passage_masks = encode_passages(text_passages,\n",
    "                                                     self.tokenizer,\n",
    "                                                     self.text_maxlength,\n",
    "                                                     self.n_context,)\n",
    "#         print(f'-----index------')\n",
    "#         pprint(index)\n",
    "#         print(f'-----target_ids------')\n",
    "#         print(target_ids.shape)\n",
    "#         pprint(target_ids)\n",
    "#         print(f'-----target_mask------')\n",
    "#         print(target_mask.shape)\n",
    "#         pprint(target_mask)\n",
    "#         print(f'-----passage_ids------')\n",
    "#         print(passage_ids.shape)\n",
    "#         pprint(passage_ids)\n",
    "#         print(f'-----passage_masks------')\n",
    "#         print(passage_masks.shape)\n",
    "#         pprint(passage_masks)\n",
    "        return (index, target_ids, target_mask, passage_ids, passage_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d7a05c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path=None, global_rank=-1, world_size=-1):\n",
    "    assert data_path\n",
    "    if data_path.endswith('.jsonl'):\n",
    "        data = open(data_path, 'r')\n",
    "    elif data_path.endswith('.json'):\n",
    "        with open(data_path, 'r') as fin:\n",
    "            data = json.load(fin)\n",
    "    examples = []\n",
    "    for k, example in enumerate(data):\n",
    "        if global_rank > -1 and not k%world_size==global_rank:\n",
    "            continue\n",
    "        if data_path is not None and data_path.endswith('.jsonl'):\n",
    "            example = json.loads(example)\n",
    "        if not 'id' in example:\n",
    "            example['id'] = k\n",
    "        for c in example['ctxs']:\n",
    "            if not 'score' in c:\n",
    "                c['score'] = 1.0 / (k + 1)\n",
    "        examples.append(example)\n",
    "    ## egrave: is this needed?\n",
    "    if data_path is not None and data_path.endswith('.jsonl'):\n",
    "        data.close()\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3cb9627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_passages(batch_text_passages, tokenizer, max_length):\n",
    "#     passage_ids, passage_masks = [], []\n",
    "#     for k, text_passages in enumerate(batch_text_passages):\n",
    "#         p = tokenizer.batch_encode_plus(\n",
    "#             text_passages,\n",
    "#             max_length=max_length,\n",
    "#             pad_to_max_length=True,\n",
    "#             return_tensors='pt',\n",
    "#             truncation=True\n",
    "#         )\n",
    "#         passage_ids.append(p['input_ids'][None])\n",
    "#         passage_masks.append(p['attention_mask'][None])\n",
    "\n",
    "#     passage_ids = torch.cat(passage_ids, dim=0)\n",
    "#     passage_masks = torch.cat(passage_masks, dim=0)\n",
    "#     return passage_ids, passage_masks.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72c9abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_passages(batch_text_passages, tokenizer, max_length, n_context):\n",
    "    passage_ids, passage_masks = [], []\n",
    "    for k, text_passages in enumerate(batch_text_passages):\n",
    "        p = tokenizer.batch_encode_plus(\n",
    "            text_passages,\n",
    "            max_length=max_length,\n",
    "            pad_to_max_length=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        cur_ctx = p['input_ids'].shape[0]\n",
    "        print(\"p['input_ids'] type\")\n",
    "        print(p['input_ids'].dtype)\n",
    "        print('--------')\n",
    "        if cur_ctx < n_context:\n",
    "            repl_lst = []\n",
    "            while cur_ctx < n_context:\n",
    "                cur_ctx += 1\n",
    "                repl_lst.append(torch.zeros_like(p['input_ids']))\n",
    "            repli_tensor = torch.cat(repl_lst, dim=0)\n",
    "            print(\"repli_tensor type\")\n",
    "            print(repli_tensor.dtype)\n",
    "            p['input_ids'] = torch.cat([p['input_ids'], repli_tensor], dim = 0)\n",
    "            \n",
    "        cur_ctx = p['attention_mask'].shape[0]\n",
    "        \n",
    "        print(\"p['attention_mask'] type\")\n",
    "        print(p['attention_mask'].dtype)\n",
    "        print('--------')\n",
    "        if cur_ctx < n_context:\n",
    "            repl_lst = []\n",
    "            while cur_ctx < n_context:\n",
    "                cur_ctx += 1\n",
    "                repl_lst.append(torch.zeros_like(p['attention_mask']))\n",
    "            repli_tensor = torch.cat(repl_lst, dim=0)\n",
    "            print(\"repli_tensor type\")\n",
    "            print(repli_tensor.dtype)\n",
    "            print('--------')\n",
    "            p['attention_mask'] = torch.cat([p['attention_mask'], repli_tensor], dim = 0)\n",
    "        \n",
    "#         print(f'cur_ctx < n_context : {cur_ctx < n_context}')\n",
    "#         print(p['input_ids'].shape)\n",
    "#         print(p['attention_mask'].shape)\n",
    "        \n",
    "        passage_ids.append(p['input_ids'][None])\n",
    "        passage_masks.append(p['attention_mask'][None])\n",
    "\n",
    "    passage_ids = torch.cat(passage_ids, dim=0)\n",
    "    passage_masks = torch.cat(passage_masks, dim=0)\n",
    "    return passage_ids, passage_masks.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03ca7874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 n_context=None,\n",
    "                 question_prefix='question:',\n",
    "                 title_prefix='title:',\n",
    "                 passage_prefix='context:'):\n",
    "        self.data = data\n",
    "        self.n_context = n_context\n",
    "        self.question_prefix = question_prefix\n",
    "        self.title_prefix = title_prefix\n",
    "        self.passage_prefix = passage_prefix\n",
    "        self.sort_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_target(self, example):\n",
    "        if 'target' in example:\n",
    "            target = example['target']\n",
    "            return target + ' </s>'\n",
    "        elif 'answers' in example:\n",
    "            return random.choice(example['answers']) + ' </s>'\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = self.data[index]\n",
    "        question = self.question_prefix + \" \" + example['question']\n",
    "        target = self.get_target(example)\n",
    "\n",
    "        if 'ctxs' in example and self.n_context is not None:\n",
    "            f = self.title_prefix + \" {} \" + self.passage_prefix + \" {}\"\n",
    "            contexts = example['ctxs'][:self.n_context]\n",
    "            passages = [f.format(c['title'], c['text']) for c in contexts]\n",
    "            scores = [float(c['score']) for c in contexts]\n",
    "            scores = torch.tensor(scores)\n",
    "            # TODO(egrave): do we want to keep this?\n",
    "            if len(contexts) == 0:\n",
    "                contexts = [question]\n",
    "        else:\n",
    "            passages, scores = None, None\n",
    "\n",
    "\n",
    "        return {\n",
    "            'index' : index,\n",
    "            'question' : question,\n",
    "            'target' : target,\n",
    "            'passages' : passages,\n",
    "            'scores' : scores\n",
    "        }\n",
    "\n",
    "    def sort_data(self):\n",
    "        if self.n_context is None or not 'score' in self.data[0]['ctxs'][0]:\n",
    "            return\n",
    "        for ex in self.data:\n",
    "            ex['ctxs'].sort(key=lambda x: float(x['score']), reverse=True)\n",
    "\n",
    "    def get_example(self, index):\n",
    "        return self.data[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc9df957",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 't5-large'\n",
    "text_maxlength = 200\n",
    "answer_maxlength = 20\n",
    "eval_data = '/data/philhoon-relevance/FiD/open_domain_data/NQ_KILT_BM25_SELECTION/strict_positive_strict_damaging_remove_damage_irrelevant_relevant.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae22f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "786d2ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_context = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4079df73",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator_function = Collator(text_maxlength, tokenizer, n_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8850036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_examples = load_data(\n",
    "        eval_data, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12cbd1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2539"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eafc72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = Dataset(\n",
    "    eval_examples, \n",
    "    n_context, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1d1a8918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# pprint(len(eval_dataset[i]['passages']))\n",
    "# pprint(eval_dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9d5f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_sampler = SequentialSampler(eval_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c867609",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, \n",
    "    sampler=eval_sampler, \n",
    "    batch_size=2,\n",
    "    collate_fn=collator_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b655eab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2c435864",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "=================================\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n",
      "p['input_ids'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "p['attention_mask'] type\n",
      "torch.int64\n",
      "--------\n",
      "repli_tensor type\n",
      "torch.int64\n",
      "--------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 5 but got size 8 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m eval_dataloader:\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#     print(a)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=================================\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/relevance-kilt/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/relevance-kilt/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/relevance-kilt/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [33], line 56\u001b[0m, in \u001b[0;36mCollator.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     51\u001b[0m             text_passages\u001b[38;5;241m.\u001b[39mappend(append_question(example))\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#         text_passages = [append_question(example) for example in batch]\u001b[39;00m\n\u001b[1;32m     53\u001b[0m         \n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#         print('text_passages')\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#         pprint(text_passages)\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m         passage_ids, passage_masks \u001b[38;5;241m=\u001b[39m \u001b[43mencode_passages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_passages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_maxlength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m#         print(f'-----index------')\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m#         pprint(index)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#         print(f'-----target_ids------')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#         print(passage_masks.shape)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m#         pprint(passage_masks)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (index, target_ids, target_mask, passage_ids, passage_masks)\n",
      "Cell \u001b[0;32mIn [36], line 49\u001b[0m, in \u001b[0;36mencode_passages\u001b[0;34m(batch_text_passages, tokenizer, max_length, n_context)\u001b[0m\n\u001b[1;32m     46\u001b[0m     passage_ids\u001b[38;5;241m.\u001b[39mappend(p[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[1;32m     47\u001b[0m     passage_masks\u001b[38;5;241m.\u001b[39mappend(p[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[0;32m---> 49\u001b[0m passage_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpassage_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m passage_masks \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(passage_masks, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m passage_ids, passage_masks\u001b[38;5;241m.\u001b[39mbool()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 5 but got size 8 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "\n",
    "for a in eval_dataloader:\n",
    "#     print(a)\n",
    "    print('=================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "68608107",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce6d72c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55f5181d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], dtype=torch.int32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.to(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "424e814f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6a3e3542",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.to(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5ddccc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], dtype=torch.int32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passage_ids = torch.tensor(passage_ids).to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc48baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_passages = [['question: which president of the united states was a boy scout title: Boy '\n",
    "  'Scouts of America context: Rowe, Steven Spielberg, Mayor Michael Bloomberg, '\n",
    "  'Secretary of Defense Robert Gates, and President Gerald Ford are just a '\n",
    "  'small sample of Eagle Scouts. Since February 2019, girls have been eligible '\n",
    "  'to earn ranks and merit badges including Eagle Scout, following the same '\n",
    "  'requirements as boys. Since the minimum time required time to earn Eagle '\n",
    "  'Scout is about 18 months, a time extension has been allowed, ostensibly to '\n",
    "  'ease the transition into the era of the first female Eagle Scout '\n",
    "  'candidates. All scouts who were older than 16 but'],\n",
    " ['question: which river separates the bronx in new york city from manhattan '\n",
    "  'island title: The Bronx context: The Bronx The Bronx is the northernmost of '\n",
    "  'the five boroughs of New York City, in the U.S. state of New York. It is '\n",
    "  'south of Westchester County; northeast and east of Manhattan, across the '\n",
    "  'Harlem River; and north of Queens, across the East River. Since 1914, the '\n",
    "  'borough has had the same boundaries as Bronx County, the third-most densely '\n",
    "  'populated county in the United States. The Bronx has a land area of and a '\n",
    "  'population of 1,471,160 in 2017. Of the five boroughs,',\n",
    "  'question: which river separates the bronx in new york city from manhattan '\n",
    "  'island title: New York City context: tidal straitflows from Long Island '\n",
    "  'Sound and separates the Bronx and Manhattan from Long Island. The Harlem '\n",
    "  'River, another tidal strait between the East and Hudson Rivers, separates '\n",
    "  'most of Manhattan from the Bronx. The Bronx River, which flows through the '\n",
    "  'Bronx and Westchester County, is the only entirely fresh water river in the '\n",
    "  \"city. The city's land has been altered substantially by human intervention, \"\n",
    "  'with considerable land reclamation along the waterfronts since Dutch '\n",
    "  'colonial times; reclamation is most prominent in Lower Manhattan, with '\n",
    "  'developments such']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82caec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446c499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 200\n",
    "# t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92140410",
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_ids, passage_masks = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5098f2d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t_passage = text_passages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabf22f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f222ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tokenizer.batch_encode_plus(\n",
    "            t_passage,\n",
    "            max_length=max_length,\n",
    "            pad_to_max_length=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6f054",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334358ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "p['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a186ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(p['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f07a0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(p['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d64590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_ = p['input_ids'][None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968edb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "p['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7c4a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replica = torch.zeros_like(p['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78302db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_contxt = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70d4289",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_ctx = p['input_ids'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bac31e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379fb768",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "repl_lst = []\n",
    "while cur_ctx < n_context:\n",
    "    cur_ctx += 1\n",
    "    print(cur_ctx)\n",
    "    input_size = torch.empty(1, p['input_ids'].shape[1])\n",
    "    replica = torch.zeros_like(input_size)\n",
    "    repl_lst.append(replica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b887dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ea1050",
   "metadata": {},
   "outputs": [],
   "source": [
    "p['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9616755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "replica.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03ddc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(repl_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3022fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repl_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da1c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "repli_tensor = torch.cat(repl_lst, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228e28aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "repli_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f76a64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merg_tensor = torch.cat([p['input_ids'], repli_tensor], dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b14aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merg_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda5028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "repli_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b5bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77c855a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merg_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497e4816",
   "metadata": {},
   "outputs": [],
   "source": [
    "p['input_ids'] = merg_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b7546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eec5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_passages_upto_n_context(batch_text_passages, tokenizer, max_length, n_context):\n",
    "    passage_ids, passage_masks = [], []\n",
    "    for k, text_passages in enumerate(batch_text_passages):\n",
    "        p = tokenizer.batch_encode_plus(\n",
    "            text_passages,\n",
    "            max_length=max_length,\n",
    "            pad_to_max_length=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        cur_ctx = p['input_ids'].shape[0]\n",
    "        if cur_ctx <= n_context:\n",
    "            repl_lst = []\n",
    "            while cur_ctx < n_context:\n",
    "                cur_ctx += 1\n",
    "                replica = torch.zeros_like(p['input_ids'])\n",
    "                repl_lst.append(replica)\n",
    "            repli_tensor = torch.cat(repl_lst, dim=0)\n",
    "            merg_tensor = torch.cat([p['input_ids'], repli_tensor], dim = 0)\n",
    "            p['input_ids'] = merg_tensor\n",
    "            \n",
    "        cur_ctx = p['attention_mask'].shape[0]\n",
    "        if cur_ctx < n_context:\n",
    "            repl_lst = []\n",
    "            while cur_ctx < n_context:\n",
    "                cur_ctx += 1\n",
    "                replica = torch.zeros_like(p['attention_mask'])\n",
    "                repl_lst.append(replica)\n",
    "            repli_tensor = torch.cat(repl_lst, dim=0)\n",
    "            merg_tensor = torch.cat([p['attention_mask'], repli_tensor], dim = 0)\n",
    "            p['attention_mask'] = merg_tensor\n",
    "        \n",
    "        passage_ids.append(p['input_ids'][None])\n",
    "        passage_masks.append(p['attention_mask'][None])\n",
    "\n",
    "    passage_ids = torch.cat(passage_ids, dim=0)\n",
    "    passage_masks = torch.cat(passage_masks, dim=0)\n",
    "    return passage_ids, passage_masks.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7780a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "replica = torch.zeros_like(p['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c0619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack((p['input_ids'], replica), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53918d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d0cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f2d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4e2000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ad2d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05251361",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack((p['input_ids'], pad_), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332edaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c675523",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7970bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d40336e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7295656",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66685868",
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_masks = p['attention_mask'][None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96880fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2ad094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_passages(batch_text_passages, tokenizer, max_length):\n",
    "    passage_ids, passage_masks = [], []\n",
    "    for k, text_passages in enumerate(batch_text_passages):\n",
    "        p = tokenizer.batch_encode_plus(\n",
    "            text_passages,\n",
    "            max_length=max_length,\n",
    "            pad_to_max_length=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        passage_ids.append(p['input_ids'][None])\n",
    "        passage_masks.append(p['attention_mask'][None])\n",
    "\n",
    "    passage_ids = torch.cat(passage_ids, dim=0)\n",
    "    passage_masks = torch.cat(passage_masks, dim=0)\n",
    "    return passage_ids, passage_masks.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197a104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_masks.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b82c92b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3c8cba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee3a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09632c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83676820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10cbdb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3332a2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51168fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9b0012",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c04e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a[1].shape)\n",
    "print(a[1].dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e08ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.squeeze(a[1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf15e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5bcbee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab5194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9e0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b5b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from util import utils\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49f712c",
   "metadata": {},
   "source": [
    "## Get dataset for incremental Test\n",
    "- README.md method 2,3\n",
    "    - Method1 describes previous ranking system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8744464e",
   "metadata": {},
   "source": [
    "## Definitions of Positive, Relevant, Damaging, Irrelevant  with respect to retriever (check out Meeting#3.ipynb)\n",
    "    - Previous Definition of relevant passages are vague.\n",
    "    - What is the definition of relevance?\n",
    "        - Passages are retrived from the step?\n",
    "    - Here we are going to dicuss the definition of each paradigm\n",
    "\n",
    "### Previous passages are divded into two categories with respect to query\n",
    "### Assumption \n",
    "    - higher the similarity score higher the the performance on downstream tasks\n",
    "        - similarity score has positive correlation with positive passage\n",
    "    - low similarity socre considered to be irrelevant degrades the output\n",
    "    - Therefore, retrieval is a process of extracting positive passage from the relevant set\n",
    "    - by shrinking the search space by top-k     \n",
    "####  1. Positive passages \n",
    "        : passages that include correct answer\n",
    "        : sometimes multiple passages are required\n",
    "#### 2. Relevant passages (Relevant set : R)\n",
    "        : passages have high possibilities in containing corret answer with repect to query/claim\n",
    "        : usually measured by similariy score\n",
    "#### 3. Irrelevant passages (Irrelevant set : I) \n",
    "        : passages have low probabilities in containing corret answer with repect to query/claim\n",
    "        : usually measured by similariy score\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b120c5c",
   "metadata": {},
   "source": [
    "### New Definitions are now divided into three categories with respect to query\n",
    "\n",
    "### Assumption\n",
    "    - Previous works focus on the re-ranking of retrieved passages, which based on the assumption that taking top-k passages with higher similarity scores will show higher performance in the downstream tasks.\n",
    "    - ; however, we have been shown that among those retrieved passages with higher similarity scores actually downgrades the final output which could have been produce the correct answer without them.\n",
    "    - So those passages are what we called damaging passges that have to be removed not just re-ranked by its score.\n",
    "\n",
    "#### 1. Positive passages \n",
    "        : passages that produce correct inference on downstream task\n",
    "        : sometimes multiple passages are required\n",
    "        \n",
    "#### 2. Relevant passages = R1\n",
    "    : passages retrieved from query that does not present adversarial effect when previous inference is correct\n",
    "    : passages with high possibility inlucde corret answer with repect to query/claim\n",
    "    : usually measured by similariy score\n",
    "    : Subset of of R\n",
    "    \n",
    "#### 3. Damaging passages = D1\n",
    "    : passages retrieved from query that produce wrong output when preivous inference is correct\n",
    "    : passages with high possibility inlucde corret answer with repect to query/claim\n",
    "    : HOWEVER it degrades the inference output\n",
    "    : usually measured by similariy score\n",
    "    : Subset of of R\n",
    "    \n",
    "#### 4. Irrelevant passages\n",
    "    : passages retrieved from query that does not change the answer when previous inference is not correct\n",
    "    : when there is preivous inference result and current output is not correct, we call it irrelevant\n",
    "    - Passages retrieved by retriever that have no impacts for inference\n",
    "    - usually measured by similariy score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b45a1",
   "metadata": {},
   "source": [
    "## Incremental Testing Setting\n",
    "\n",
    "### 3 Method (Check out README.md)\n",
    "\n",
    "### Method 2 (2nd method of in README.md)\n",
    "    - The sooner, The better approach\n",
    "      - Keep the Positive Context in order\n",
    "    - Since we testing it, let's keep sample size of 5\n",
    "    - When there is no Exact Mathcing during the incremental inference e.g.) em_pattern = '00000'\n",
    "        - Keep the whole context so that those cases will have False on Exact Match values\n",
    "        \n",
    "    - Patterns: \n",
    "        - first 1 : positive\n",
    "        - first 0 : irrelevant \n",
    "            - we know that first is the positive context that includes answer\n",
    "            - with that perspective, it is relevant in theory\n",
    "            - ; however, dataset is created via BLEU score on two different corpus.\n",
    "            - If FiD does not correctly infer the output, \n",
    "            - context what we concieved of artificial positive context is actually irrelevant to the query.\n",
    "            - Also when realistic scenario, we don't know whether the first context contains the answer\n",
    "        - 01 pattern : Positive \n",
    "        - 11 pattern : Strict Positive(relevant) or Naive Positive(positive)\n",
    "        - 00 pattern : Strict Damaging(irrelevant) or Naive Damaging(damaging) \n",
    "        - 10 pattern : Damaging\n",
    "        \n",
    "        - 11 pattern : relevant vs positive\n",
    "            Strict Positive\n",
    "                Under strict rule : consider it as relevant ?(or irrelevant?)\n",
    "                                in terms of strictly limiting the number of positive passages\n",
    "            Naive Positive\n",
    "                Under naive rule : consider it as positive\n",
    "                                in that this would increase the number of positive passages\n",
    "                \n",
    "        - A00 pattern : irrelevant vs damaging\n",
    "            if '1' does not occured in A, currnet passage is irrelevant\n",
    "            if '1' occurred in A, current passage is damaging either irrelevant \n",
    "                Strict Damaging\n",
    "                    Under strict rule : consider it as irrelevnat \n",
    "                                    in terms of strictly limiting the number of damaging passage\n",
    "                Naive Damaging\n",
    "                    Under naive rule : consider it as damaging \n",
    "                                    in that this would increase the number of damaging passages\n",
    "        \n",
    "    - Options1 : Removes only damaging\n",
    "    - Options2 : Removes damaging + irrelevant\n",
    "    - Options3 : Removes damaging + relevant\n",
    "    - Options4 : Removes damaging + irrelevant + relevnat\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e426e94",
   "metadata": {},
   "source": [
    "### Total Trials 16\n",
    "    1st Trial - Options1(Remove only Damaging) + Strict Positive + Strict Damaging\n",
    "    2nd Trial - Options1(Remove only Damaging) + Strict Positive + Naive Damaging\n",
    "    3rd Trial - Options1(Remove only Damaging) + Naive Positive + Strict Damaging\n",
    "    4th Trial - Options1(Remove only Damaging) + Naive Positive + Naive Damaging\n",
    "    \n",
    "    5th Trial - Options2(Remove damaging + irrelevant) + Strict Positive + Strict Damaging\n",
    "    6th Trial - Options2(Remove damaging + irrelevant) + Strict Positive + Naive Damaging\n",
    "    7th Trial - Options2(Remove damaging + irrelevant) + Naive Positive + Strict Damaging\n",
    "    8th Trial - Options2(Remove damaging + irrelevant) + Naive Positive + Naive Damaging\n",
    "    \n",
    "    9th Trial - Options3(Removes damaging + relevant) + Strict Positive + Strict Damaging\n",
    "    10th Trial - Options3(Removes damaging + relevant) + Strict Positive + Naive Damaging\n",
    "    11th Trial - Options3(Removes damaging + relevant) + Naive Positive + Strict Damaging\n",
    "    12th Trial - Options3(Removes damaging + relevant) + Naive Positive + Naive Damaging\n",
    "    \n",
    "    13th Trial - Options4(Removes damaging + irrelevant + relevant) + Strict Positive + Strict Damaging\n",
    "    14th Trial - Options4(Removes damaging + irrelevant + relevant) + Strict Positive + Naive Damaging\n",
    "    15th Trial - Options4(Removes damaging + irrelevant + relevant) + Naive Positive + Strict Damaging\n",
    "    16th Trial - Options4(Removes damaging + irrelevant + relevant) + Naive Positive + Naive Damaging\n",
    "        \n",
    "        \n",
    "    - Since Options4 removes  \"damaging + irrelevant + relevant\"\n",
    "        - there will be no difference between Strict Damaging and Naive Damaging\n",
    "\n",
    "    13, 14th Trial -> Options4(Removes damaging + irrelevant + relevant) + Strict Positive\n",
    "    15, 16th Trial -> Options4(Removes damaging + irrelevant + relevant) + Naive Positive\n",
    "    \n",
    "    - Since Options2 removes  \"damaging + irrelevant\"\n",
    "        - there will be no difference in terms of the input between Strict Damaging and Naive Damaging\n",
    "    5, 6th Trial -> Options2(Remove damaging + irrelevant) + Strict Positive -> new 5th\n",
    "    7, 8th Trial -> Options2(Remove damaging + irrelevant) + Naive Positive -> new 6th\n",
    "    - Since Options2 include both \"positive + relevant\"\n",
    "        - There will be no difference in terms of the input between Strict Positive and Naive Positive\n",
    "        - I found out during checking\n",
    "    new 5th, new 6th -> Options2(Remove damaging + irrelevant) \n",
    "    \n",
    "    - Options4(Removes damaging + irrelevant + relevant) + Strict Positive\n",
    "        - means one positive only thus same as \"pos1_ctx1\" -> nope there might be non-consecutives\n",
    "        - e.g.) 01011 -> 2 strict positive cases\n",
    "        - BUT \"pos1_ctx1\" = baseline could be our baselines\n",
    "    Options4(Removes damaging + irrelevant + relevant) + Strict Positive -> equal to \"pos1_ctx1\" \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d1686d",
   "metadata": {},
   "source": [
    "    \n",
    "### Final Total Trials 11\n",
    "    1st Trial - Options1(Remove only Damaging) + Strict Positive + Strict Damaging\n",
    "    2nd Trial - Options1(Remove only Damaging) + Strict Positive + Naive Damaging\n",
    "    3rd Trial - Options1(Remove only Damaging) + Naive Positive + Strict Damaging\n",
    "    4th Trial - Options1(Remove only Damaging) + Naive Positive + Naive Damaging\n",
    "    \n",
    "    5th Trial - Options2(Remove damaging + irrelevant) + Strict Positive + Strict Damaging \n",
    "        - The one also represents other three cases\n",
    "            - Options2(Remove damaging + irrelevant) + Strict Positive + Naive Damaging\n",
    "            - Options2(Remove damaging + irrelevant) + Naive Positive + Strict Damaging\n",
    "            - Options2(Remove damaging + irrelevant) + Naive Positive + Naive Damaging\n",
    "    \n",
    "    6th Trial - Options3(Removes damaging + relevant) + Strict Positive + Strict Damaging\n",
    "    7th Trial - Options3(Removes damaging + relevant) + Strict Positive + Naive Damaging\n",
    "    8th Trial - Options3(Removes damaging + relevant) + Naive Positive + Strict Damaging\n",
    "    9th Trial - Options3(Removes damaging + relevant) + Naive Positive + Naive Damaging\n",
    "    \n",
    "    10th Trial - Options4(Removes damaging + irrelevant + relevant) + Strict Positive \n",
    "        - The one represents cases \n",
    "            - Options4(Removes damaging + irrelevant + relevant) + Strict Positive + Strict Damaging\n",
    "            - Options4(Removes damaging + irrelevant + relevant) + Strict Positive + Naive Damaging\n",
    "        - almost equal to \"pos1_ctx1\" e.g.) 01011 -> 2 strict positive cases\n",
    "        - \"pos1_ctx1\" = baseline could be our baselines\n",
    "        \n",
    "    11th Trial - Options4(Removes damaging + irrelevant + relevant) + Naive Positive    \n",
    "        - The one represents cases \n",
    "            - Options4(Removes damaging + irrelevant + relevant) + Naive Positive + Strict Damaging\n",
    "            - Options4(Removes damaging + irrelevant + relevant) + Naive Positive + Naive Damaging\n",
    "    \n",
    "    \n",
    "### Estimation \n",
    "    - Since retrieved passages have high similarity score, retrieved passages are higly likely to form damaging \n",
    "    passages with repect to query. \n",
    "    - Minimizing the risk of exposing query to damaging passages would be favorabe to FiD\n",
    "    - The simplest way to reduce to the probability of such occurences is limiting the number of input passages\n",
    "\n",
    "    - I think below strategy will hold the best result \n",
    "        1) Options2(Remove damaging + irrelevant)\n",
    "            * As we saw from result from FiD result with Random Sampling, \n",
    "            * FiD is powerful distinguish the positive from irrelevants.\n",
    "            * Even though we loose stronger boundary that might be helpful to inference, \n",
    "            * It will be (frivolous/trivial/not consequential)\n",
    "        2) Strict Positive (relevant) \n",
    "            * Strict Positive puts a rigorous boundary for passages, meaning less positive passages\n",
    "        3) Naive Damaging(damaging) \n",
    "            * Unlike Strict Positive, we can set up a lenient standard for damaging to lessen the size of input. \n",
    "            * Increasing Damaging passages by definition results in decreasing number of inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3305859",
   "metadata": {},
   "source": [
    "### Method2\n",
    "    - Same approach but in reciprocal order\n",
    "      - We know that FiD is order invariant but this is for checking \n",
    "      - don't need to test on whole trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f3933d",
   "metadata": {},
   "source": [
    "# Input\n",
    "    - result from 5-1\n",
    "    - /data/philhoon-relevance/FiD/results/KILT_BM25_NQ/incremental_result/pos1_ctx5.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb2e864",
   "metadata": {},
   "source": [
    "# Trials\n",
    "    - Need to check FiD input when there are less ctxs than n_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2926bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = utils.open_json('/data/philhoon-relevance/FiD/results/KILT_BM25_NQ/incremental_result/pos1_ctx5.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b33e7",
   "metadata": {},
   "source": [
    "### format\n",
    "```python\n",
    "{\n",
    "    'id' : str()\n",
    "    'answers' : list()\n",
    "    'ctxs' : list(dict)\n",
    "    'questions' : str()\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c594cc4b",
   "metadata": {},
   "source": [
    "#### Test Code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1949e2b4",
   "metadata": {},
   "source": [
    "### Checking on Final Total Trials 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83095e97",
   "metadata": {},
   "source": [
    "    1st Trial - Options1(Remove only Damaging) + Strict Positive + Strict Damaging (checked)\n",
    "    2nd Trial - Options1(Remove only Damaging) + Strict Positive + Naive Damaging (checked)\n",
    "    3rd Trial - Options1(Remove only Damaging) + Naive Positive + Strict Damaging (checked)\n",
    "    4th Trial - Options1(Remove only Damaging) + Naive Positive + Naive Damaging (checked)\n",
    "    \n",
    "    5th Trial - Options2(Remove damaging + irrelevant) + Strict Positive + Strict Damaging (checked)\n",
    "        - The one also represents other three cases\n",
    "            - Options2(Remove damaging + irrelevant) + Strict Positive + Naive Damaging \n",
    "            - Options2(Remove damaging + irrelevant) + Naive Positive + Strict Damaging (checked)\n",
    "            - Options2(Remove damaging + irrelevant) + Naive Positive + Naive Damaging \n",
    "    \n",
    "    6th Trial - Options3(Removes damaging + relevant) + Strict Positive + Strict Damaging (checked)\n",
    "    7th Trial - Options3(Removes damaging + relevant) + Strict Positive + Naive Damaging (checked)\n",
    "    8th Trial - Options3(Removes damaging + relevant) + Naive Positive + Strict Damaging (checked)\n",
    "    9th Trial - Options3(Removes damaging + relevant) + Naive Positive + Naive Damaging (checked)\n",
    "    \n",
    "    10th Trial - Options4(Removes damaging + irrelevant + relevant) + Strict Positive (checked)\n",
    "        - The one represents cases \n",
    "            - Options4(Removes damaging + irrelevant + relevant) + Strict Positive + Strict Damaging (checked)\n",
    "            - Options4(Removes damaging + irrelevant + relevant) + Strict Positive + Naive Damaging\n",
    "        - almost equal to \"pos1_ctx1\" e.g.) 01011 -> 2 strict positive cases\n",
    "        - \"pos1_ctx1\" = baseline could be our baselines\n",
    "        \n",
    "    11th Trial - Options4(Removes damaging + irrelevant + relevant) + Naive Positive (checked)  \n",
    "        - The one represents cases \n",
    "            - Options4(Removes damaging + irrelevant + relevant) + Naive Positive + Strict Damaging\n",
    "            - Options4(Removes damaging + irrelevant + relevant) + Naive Positive + Naive Damaging (checked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53acf54",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# # cnt_conv = 0\n",
    "# # empty_temp = {\n",
    "# #     'psg_id' : None,\n",
    "# #     'text' : None,\n",
    "# #     'title' : None\n",
    "# # }\n",
    "\n",
    "# output_format = []\n",
    "\n",
    "# # 'strict', 'naive'  \n",
    "# # i = 0\n",
    "# option_p = 'naive'\n",
    "# option_d = 'naive'\n",
    "# option = 'op4'\n",
    "\n",
    "# for instance in input_file:\n",
    "#     template_dict = {}\n",
    "#     template_dict['id'] = instance['id']\n",
    "#     template_dict['answers'] = instance['answers']\n",
    "#     template_dict['question'] = instance['question']\n",
    "#     template_dict['em_pattern'] = instance['em_pattern']\n",
    "                                   \n",
    "#     em_pattern = instance['em_pattern']\n",
    "    \n",
    "# #     i += 1\n",
    "# #     if i == 3000:\n",
    "# #         break\n",
    "#     print(f'em_pattern : {em_pattern}')\n",
    "#     # when there is at least one EM in the accumulated inference\n",
    "#     if em_pattern != '00000':   \n",
    "#         new_ctx = []\n",
    "        \n",
    "#         # relevant vs positive\n",
    "#         positve_ctx_lst = []\n",
    "#         relevant_ctx_lst = []\n",
    "        \n",
    "#         # irrelevant vs damaging\n",
    "#         damaging_ctx_lst = []\n",
    "#         irrelevant_ctx_lst = []\n",
    "        \n",
    "#         # print\n",
    "#         # matchint each em to ctx\n",
    "#         for ind_em, ctx in zip(em_pattern, instance['ctxs']):\n",
    "#             print(f'{ind_em} : \\t {ctx}')\n",
    "        \n",
    "#         for idx_, ctx in enumerate(instance['ctxs']):\n",
    "            \n",
    "#             # checking current em\n",
    "#             cur_em = em_pattern[idx_]\n",
    "#             pre_em_pattern = em_pattern[:idx_]\n",
    "# #             print('-----------')\n",
    "# #             print(f'em_pattern : {em_pattern}')\n",
    "# #             print(f'idx_ : {idx_}')\n",
    "# #             print(f'cur_em : {cur_em}')\n",
    "# #             print(f'pre_em_pattern : {pre_em_pattern}')\n",
    "# #             print(f'not pre_em_pattern : {not pre_em_pattern}')\n",
    "# #             print('-----------')\n",
    "            \n",
    "#             # first 1 : positive\n",
    "#             if not pre_em_pattern and cur_em == '1':\n",
    "#                 positve_ctx_lst.append(ctx)\n",
    "                \n",
    "#                 # print\n",
    "#                 print('-----------')\n",
    "#                 print(f'em_pattern : {em_pattern}')\n",
    "#                 print(f'idx_ : {idx_}')\n",
    "#                 print(f'cur_em : {cur_em}')\n",
    "#                 print(f'1 first positive ctx : ')\n",
    "#                 pprint(ctx)\n",
    "#                 print('-----------')\n",
    "                \n",
    "#             # first 0 : irrelevant\n",
    "#             elif not pre_em_pattern and cur_em == '0':\n",
    "#                 irrelevant_ctx_lst.append(ctx)\n",
    "                \n",
    "#                 # print\n",
    "#                 print('-----------')\n",
    "#                 print(f'em_pattern : {em_pattern}')\n",
    "#                 print(f'idx_ : {idx_}')\n",
    "#                 print(f'cur_em : {cur_em}')\n",
    "#                 print(f'0 first irrelevant ctx : \\t ')\n",
    "#                 pprint(ctx)\n",
    "#                 print('-----------')\n",
    "                \n",
    "#             # 01 pattern : positive \n",
    "#             elif pre_em_pattern and pre_em_pattern[-1] == '0' and cur_em == '1':\n",
    "#                 positve_ctx_lst.append(ctx)\n",
    "                \n",
    "#                 # print\n",
    "#                 print('-----------')\n",
    "#                 print(f'em_pattern : {em_pattern}')\n",
    "#                 print(f'idx_ : {idx_}')\n",
    "#                 print(f'cur_em : {cur_em}')\n",
    "#                 print(f'01 pattern : positive  : \\t ')\n",
    "#                 pprint(ctx)\n",
    "#                 print('-----------')\n",
    "                \n",
    "#             # 10 pattern : damaging\n",
    "#             elif pre_em_pattern and pre_em_pattern[-1] == '1' and cur_em == '0':\n",
    "#                 damaging_ctx_lst.append(ctx)\n",
    "                \n",
    "#                 # print\n",
    "#                 print('-----------')\n",
    "#                 print(f'em_pattern : {em_pattern}')\n",
    "#                 print(f'idx_ : {idx_}')\n",
    "#                 print(f'cur_em : {cur_em}')\n",
    "#                 print(f'10 pattern : damaging : \\t ')\n",
    "#                 pprint(ctx)\n",
    "#                 print('-----------')\n",
    "                \n",
    "#             # 11 pattern : Strict Positive(relevant) or Naive Positive(positive)\n",
    "#             elif pre_em_pattern and pre_em_pattern[-1] == '1' and cur_em == '1':\n",
    "#                 if option_p == 'strict':\n",
    "#                     relevant_ctx_lst.append(ctx)\n",
    "                    \n",
    "#                     # print\n",
    "#                     print('-----------')\n",
    "#                     print(f'em_pattern : {em_pattern}')\n",
    "#                     print(f'idx_ : {idx_}')\n",
    "#                     print(f'cur_em : {cur_em}')\n",
    "#                     print(f'11 pattern : strict : relevant \\t ')\n",
    "#                     pprint(ctx)\n",
    "#                     print('-----------')\n",
    "                \n",
    "#                 elif option_p == 'naive':\n",
    "#                     positve_ctx_lst.append(ctx)\n",
    "                    \n",
    "#                     # print\n",
    "#                     print('-----------')\n",
    "#                     print(f'em_pattern : {em_pattern}')\n",
    "#                     print(f'idx_ : {idx_}')\n",
    "#                     print(f'cur_em : {cur_em}')\n",
    "#                     print(f'11 pattern : naive : positive \\t ')\n",
    "#                     pprint(ctx)\n",
    "#                     print('-----------')\n",
    "                    \n",
    "#                 else:\n",
    "#                     print('option_p should be either \\'strict\\' or \\'naive\\'')\n",
    "#                     break\n",
    "                    \n",
    "#             # 00 pattern : Strict Damaging(irrelevant) or Naive Damaging(damaging) \n",
    "#             elif pre_em_pattern and pre_em_pattern[-1] == '0' and cur_em == '0':\n",
    "#                 # if '1' does not occured in A, currnet passage is irrelevant\n",
    "#                 if not '1' in pre_em_pattern:\n",
    "#                     irrelevant_ctx_lst.append(ctx)\n",
    "                    \n",
    "#                     # print\n",
    "#                     print('-----------')\n",
    "#                     print(f'em_pattern : {em_pattern}')\n",
    "#                     print(f'idx_ : {idx_}')\n",
    "#                     print(f'cur_em : {cur_em}')\n",
    "#                     print(f'00 pattern : 1 does not exist : irrelevant_ctx_lst \\t ')\n",
    "#                     pprint(ctx)\n",
    "#                     print('-----------')\n",
    "#                 # if '1' occurred in A, \n",
    "#                 else:\n",
    "#                     # strict : consider it as irrelevnat \n",
    "#                     if option_d == 'strict':\n",
    "#                         irrelevant_ctx_lst.append(ctx)\n",
    "#                         # print\n",
    "#                         print('-----------')\n",
    "#                         print(f'em_pattern : {em_pattern}')\n",
    "#                         print(f'idx_ : {idx_}')\n",
    "#                         print(f'cur_em : {cur_em}')\n",
    "#                         print(f'00 pattern : 1 exists : strict :irrelevant_ctx_lst \\t ')\n",
    "#                         pprint(ctx)\n",
    "#                         print('-----------')\n",
    "#                     # naive : consider it as damaging \n",
    "#                     elif option_d == 'naive':\n",
    "#                         damaging_ctx_lst.append(ctx)\n",
    "#                         # print\n",
    "#                         print('-----------')\n",
    "#                         print(f'em_pattern : {em_pattern}')\n",
    "#                         print(f'idx_ : {idx_}')\n",
    "#                         print(f'cur_em : {cur_em}')\n",
    "#                         print(f'00 pattern : 1 exists : naive :damaging_ctx_lst \\t ')\n",
    "#                         pprint(ctx)\n",
    "#                         print('-----------')\n",
    "#                     else:\n",
    "#                         print('option_d should be either \\'strict\\' or \\'naive\\'')\n",
    "#                         break\n",
    "                    \n",
    "#         # op1 removes damages only\n",
    "#         if option == 'op1':\n",
    "#             new_ctx.extend(positve_ctx_lst)\n",
    "#             new_ctx.extend(relevant_ctx_lst)\n",
    "#             new_ctx.extend(irrelevant_ctx_lst)\n",
    "            \n",
    "#             print(f'option : {option}')\n",
    "#             print(f'removes damages only')\n",
    "#             print(f'damages')\n",
    "#             pprint(damaging_ctx_lst)\n",
    "#             print(f'rest_ctx')\n",
    "#             pprint(new_ctx)\n",
    "            \n",
    "#         # op2 removes damaging + irrelevant\n",
    "#         elif option == 'op2':\n",
    "#             new_ctx.extend(positve_ctx_lst)\n",
    "#             new_ctx.extend(relevant_ctx_lst)\n",
    "            \n",
    "#             print(f'option : {option}')\n",
    "#             print(f'removes removes damaging + irrelevant')\n",
    "#             print(f'damages')\n",
    "#             pprint(damaging_ctx_lst)\n",
    "#             print(f'irrelevant')\n",
    "#             pprint(irrelevant_ctx_lst)\n",
    "#             print(f'rest_ctx')\n",
    "#             pprint(new_ctx)\n",
    "            \n",
    "#         # op3 : Removes damaging + relevant\n",
    "#         elif option == 'op3':\n",
    "#             new_ctx.extend(positve_ctx_lst)\n",
    "#             new_ctx.extend(irrelevant_ctx_lst)\n",
    "            \n",
    "#             print(f'option : {option}')\n",
    "#             print(f'removes removes damaging + relevant')\n",
    "#             print(f'damages')\n",
    "#             pprint(damaging_ctx_lst)\n",
    "#             print(f'relevant_ctx_lst')\n",
    "#             pprint(relevant_ctx_lst)\n",
    "#             print(f'rest_ctx')\n",
    "#             pprint(new_ctx)\n",
    "            \n",
    "#         # op4 : Removes damaging + irrelevant + relevant\n",
    "#         elif option == 'op4':\n",
    "#             new_ctx.extend(positve_ctx_lst)\n",
    "            \n",
    "#             print(f'option : {option}')\n",
    "#             print(f'removes removes damaging + irrelevant + relevant')\n",
    "#             print(f'damages')\n",
    "#             pprint(damaging_ctx_lst)\n",
    "#             print(f'irrelevant')\n",
    "#             pprint(irrelevant_ctx_lst)\n",
    "#             print(f'relevant_ctx_lst')\n",
    "#             pprint(relevant_ctx_lst)\n",
    "#             print(f'rest_ctx')\n",
    "#             pprint(new_ctx)\n",
    "            \n",
    "#         else:\n",
    "#             print('option should be op1, op2, op3, op4')\n",
    "#             break\n",
    "        \n",
    "#         template_dict['ctxs'] = new_ctx\n",
    "#         output_format.append(template_dict)\n",
    "        \n",
    "#     # when there is no EM in the accumulated inference\n",
    "#     else:\n",
    "#         print(f'em_pattern == 00000')\n",
    "#         template_dict['ctxs']= instance['ctxs']\n",
    "#         output_format.append(template_dict)\n",
    "        \n",
    "    \n",
    "#     print('==============instance finished======================')\n",
    "# #         template_dict['ctxs']= new_ctx\n",
    "# #         output_format.append(template_dict)\n",
    "        \n",
    "# #     else:\n",
    "# #         template_dict['ctxs']= instance['ctxs']\n",
    "# #         output_format.append(template_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd00940",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa293b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(input_file, option, option_p, option_d):\n",
    "    '''\n",
    "    input_file : incremental inference result from FiD from KILT-5-1\n",
    "        path : /data/philhoon-relevance/FiD/results/KILT_BM25_NQ/incremental_result/pos1_ctx5.json\n",
    "        \n",
    "    output : FiD input json format\n",
    "    \n",
    "    option(required) : removing strategies\n",
    "        op1 : removes damages only\n",
    "        op2 : removes damaging + irrelevant\n",
    "        op3 : removes damaging + relevant\n",
    "        op4 : removes damaging + irrelevant + relevant\n",
    "        \n",
    "    option_p(required) : positive passage selection options\n",
    "        strict : strict positive\n",
    "            e.g.) 11 pattern \n",
    "                1st '1' is positive, 2nd '1' is relevant\n",
    "        naive : naive positive\n",
    "            e.g.) 11 pattern \n",
    "                1st '1' is positive, 2nd '1' is positive\n",
    "                \n",
    "    option_d(required) : damaging passage selection options\n",
    "        strict : strict negative\n",
    "            e.g.) A00 pattern \n",
    "                if there is at least one '1' occurred in A, 2nd '0' is irrelevant\n",
    "        naive : naive damaging\n",
    "            e.g.) A00 pattern \n",
    "                if there is at least one '1' occurred in A, 2nd '0' is damaging\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    output_format = []\n",
    "\n",
    "    # 'strict', 'naive'  \n",
    "    # option_p = 'naive'\n",
    "    # option_d = 'naive'\n",
    "    # option = 'op4'\n",
    "\n",
    "    for instance in input_file:\n",
    "        template_dict = {}\n",
    "        template_dict['id'] = instance['id']\n",
    "        template_dict['answers'] = instance['answers']\n",
    "        template_dict['question'] = instance['question']\n",
    "        template_dict['em_pattern'] = instance['em_pattern']\n",
    "\n",
    "        em_pattern = instance['em_pattern']\n",
    "\n",
    "        # when there is at least one EM in the accumulated inference\n",
    "        if em_pattern != '00000':   \n",
    "            new_ctx = []\n",
    "\n",
    "            # relevant vs positive\n",
    "            positve_ctx_lst = []\n",
    "            relevant_ctx_lst = []\n",
    "\n",
    "            # irrelevant vs damaging\n",
    "            damaging_ctx_lst = []\n",
    "            irrelevant_ctx_lst = []\n",
    "\n",
    "\n",
    "            for idx_, ctx in enumerate(instance['ctxs']):\n",
    "\n",
    "                # checking current em\n",
    "                cur_em = em_pattern[idx_]\n",
    "                pre_em_pattern = em_pattern[:idx_]\n",
    "\n",
    "\n",
    "                # first 1 : positive\n",
    "                if not pre_em_pattern and cur_em == '1':\n",
    "                    positve_ctx_lst.append(ctx)\n",
    "\n",
    "                # first 0 : irrelevant\n",
    "                elif not pre_em_pattern and cur_em == '0':\n",
    "                    irrelevant_ctx_lst.append(ctx)\n",
    "                    \n",
    "                # 01 pattern : positive \n",
    "                elif pre_em_pattern and pre_em_pattern[-1] == '0' and cur_em == '1':\n",
    "                    positve_ctx_lst.append(ctx)\n",
    "\n",
    "                # 10 pattern : damaging\n",
    "                elif pre_em_pattern and pre_em_pattern[-1] == '1' and cur_em == '0':\n",
    "                    damaging_ctx_lst.append(ctx)\n",
    "\n",
    "                # 11 pattern : Strict Positive(relevant) or Naive Positive(positive)\n",
    "                elif pre_em_pattern and pre_em_pattern[-1] == '1' and cur_em == '1':\n",
    "                    if option_p == 'strict':\n",
    "                        relevant_ctx_lst.append(ctx)\n",
    "\n",
    "                    elif option_p == 'naive':\n",
    "                        positve_ctx_lst.append(ctx)\n",
    "\n",
    "                    else:\n",
    "                        print('option_p should be either \\'strict\\' or \\'naive\\'')\n",
    "                        return \n",
    "\n",
    "                # 00 pattern : Strict Damaging(irrelevant) or Naive Damaging(damaging) \n",
    "                elif pre_em_pattern and pre_em_pattern[-1] == '0' and cur_em == '0':\n",
    "                    # if '1' does not occured in A, currnet passage is irrelevant\n",
    "                    if not '1' in pre_em_pattern:\n",
    "                        irrelevant_ctx_lst.append(ctx)\n",
    "\n",
    "                    # if '1' occurred in A, \n",
    "                    else:\n",
    "                        # strict : consider it as irrelevnat \n",
    "                        if option_d == 'strict':\n",
    "                            irrelevant_ctx_lst.append(ctx)\n",
    "\n",
    "                        # naive : consider it as damaging \n",
    "                        elif option_d == 'naive':\n",
    "                            damaging_ctx_lst.append(ctx)\n",
    "\n",
    "                        else:\n",
    "                            print('option_p should be either \\'strict\\' or \\'naive\\'')\n",
    "                            return \n",
    "\n",
    "            # op1 removes damages only\n",
    "            if option == 'op1':\n",
    "                new_ctx.extend(positve_ctx_lst)\n",
    "                new_ctx.extend(relevant_ctx_lst)\n",
    "                new_ctx.extend(irrelevant_ctx_lst)\n",
    "\n",
    "\n",
    "            # op2 removes damaging + irrelevant\n",
    "            elif option == 'op2':\n",
    "                new_ctx.extend(positve_ctx_lst)\n",
    "                new_ctx.extend(relevant_ctx_lst)\n",
    "\n",
    "            # op3 : Removes damaging + relevant\n",
    "            elif option == 'op3':\n",
    "                new_ctx.extend(positve_ctx_lst)\n",
    "                new_ctx.extend(irrelevant_ctx_lst)\n",
    "\n",
    "            # op4 : Removes damaging + irrelevant + relevant\n",
    "            elif option == 'op4':\n",
    "                new_ctx.extend(positve_ctx_lst)\n",
    "\n",
    "            else:\n",
    "                print('option should be op1, op2, op3, op4')\n",
    "                return \n",
    "\n",
    "            template_dict['ctxs'] = new_ctx\n",
    "            output_format.append(template_dict)\n",
    "\n",
    "        # when there is no EM in the accumulated inference\n",
    "        else:\n",
    "            template_dict['ctxs']= instance['ctxs']\n",
    "            output_format.append(template_dict)\n",
    "    \n",
    "    print('==============instance finished======================')\n",
    "    return output_format\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8c270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "option_dict = {\n",
    "    'op1' : 'remove_damage',\n",
    "    'op2' : 'remove_damage_irrelevant',\n",
    "    'op3' : 'remove_damage_relevant',\n",
    "    'op4' : 'remove_damage_irrelevant_relevant',\n",
    "}\n",
    "# option = 'op4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8817b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "option_p_dict = {\n",
    "    'strict' : 'strict_positive',\n",
    "    'naive' : 'naive_positive',\n",
    "}\n",
    "# option_p = 'strict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5403cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "option_d_dict = {\n",
    "    'strict' : 'strict_damaging',\n",
    "    'naive' : 'naive_damaging',\n",
    "}\n",
    "# option_d = 'strict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55332887",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = utils.open_json('/data/philhoon-relevance/FiD/results/KILT_BM25_NQ/incremental_result/pos1_ctx5.json')\n",
    "output_path = '/data/philhoon-relevance/FiD/open_domain_data/NQ_KILT_BM25_SELECTION'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d21998",
   "metadata": {},
   "outputs": [],
   "source": [
    "for o_ in option_dict.keys():\n",
    "    for op in option_p_dict.keys():\n",
    "        for od in option_d_dict.keys():\n",
    "            option = o_\n",
    "            option_p = op\n",
    "            option_d = od\n",
    "            \n",
    "            filename = f'{option_p_dict[option_p]}_{option_d_dict[option_d]}_{option_dict[option]}.json'\n",
    "            \n",
    "            output_file = os.path.join(output_path, filename)\n",
    "            output_format = build_data(input_file, option, option_p, option_d)\n",
    "            \n",
    "            utils.save_json(output_format, output_file)\n",
    "            print(f'{filename} save on \\n {output_path}')\n",
    "#             print(f'option : {option}')\n",
    "#             print(f'option_p : {option_p}')\n",
    "#             print(f'option_d : {option_d}')\n",
    "#             print(f'filename : {filename}')\n",
    "#             print(f'output_file : {output_file}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38040474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relevance-kilt",
   "language": "python",
   "name": "relevance-kilt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
